{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OgzHWN5i9ve"
   },
   "source": [
    "### Open questions\n",
    "\n",
    "Open questions:\n",
    "\n",
    "1. How to normilize. During center point loss do we devide per all objects in the sequence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8ti8nMotJ3I"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "kWCyXD-33WhU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCenterNetWithLSTM(nn.Module):\n",
    "    def __init__(self, num_objects=5, num_classes=10, lstm_hidden_size=64):\n",
    "        super(SimpleCenterNetWithLSTM, self).__init__()\n",
    "        self.num_objects = num_objects\n",
    "        self.num_classes = num_classes\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Fully connected layers for feature extraction\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "\n",
    "        # LSTM layer to capture temporal dependencies\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=128,  # Input size from fc1 layer\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=False\n",
    "        )\n",
    "\n",
    "        self.hidden_h0 = nn.Parameter(torch.randn(1, 1, self.lstm_hidden_size))\n",
    "        self.hidden_c0 = nn.Parameter(torch.randn(1, 1, self.lstm_hidden_size))\n",
    "\n",
    "        # Output layers after LSTM\n",
    "        self.fc_temporal = nn.Linear(lstm_hidden_size, 128)\n",
    "\n",
    "        # Output layers for center points and class scores\n",
    "        self.fc_center = nn.Linear(128, 2 * num_objects)  # Predicts (x, y) for each object\n",
    "        self.fc_class = nn.Linear(128, num_objects * num_classes)  # Predicts class scores for each object\n",
    "\n",
    "    def forward(self, samples, targets):\n",
    "\n",
    "        samples = samples.permute(1, 0, 2, 3, 4)  # change dimension order from BT___ to TB___\n",
    "\n",
    "        out_logits = []\n",
    "        out_center_points = []\n",
    "\n",
    "        # Initialize LSTM hidden state\n",
    "        batch_size = samples.size(1)\n",
    "        h0 = self.hidden_h0.expand(1, batch_size, self.lstm_hidden_size).contiguous() # [num_layers, batch_size, hidden_size]\n",
    "        c0 = self.hidden_c0.expand(1, batch_size, self.lstm_hidden_size).contiguous() # [num_layers, batch_size, hidden_size]\n",
    "\n",
    "        # Temporal feature extraction\n",
    "        temporal_features = []\n",
    "\n",
    "        targets = targets[0] # We have an assumption that batch size is 1\n",
    "\n",
    "        for timestamp, batch in enumerate(samples):\n",
    "            keep_frame = targets[timestamp]['keep_frame'].item()\n",
    "\n",
    "            if keep_frame:\n",
    "                # Forward pass through conv layers\n",
    "                x = F.relu(self.conv1(batch))\n",
    "                x = F.max_pool2d(x, 2)\n",
    "                x = F.relu(self.conv2(x))\n",
    "                x = F.max_pool2d(x, 2)\n",
    "                x = F.relu(self.conv3(x))\n",
    "                x = F.max_pool2d(x, 2)\n",
    "                \n",
    "                # Flatten and apply fully connected layer\n",
    "                x = x.view(x.size(0), -1)  # Flatten\n",
    "                \n",
    "                x = F.relu(self.fc1(x))\n",
    "                \n",
    "            else:\n",
    "                x = torch.zeros(1, 128).to(batch.device)\n",
    "\n",
    "            temporal_features.append(x)\n",
    "\n",
    "        # Stack temporal features\n",
    "        temporal_features = torch.stack(temporal_features)\n",
    "\n",
    "        # Pass through LSTM\n",
    "        lstm_out, _ = self.lstm(temporal_features, (h0, c0))\n",
    "\n",
    "        # Process LSTM output for each time step\n",
    "        for t in range(lstm_out.size(0)):\n",
    "            # Apply temporal feature transformation\n",
    "            x = F.relu(self.fc_temporal(lstm_out[t]))\n",
    "\n",
    "            # Predict centers\n",
    "            center_output = self.fc_center(x)  # Output shape: (batch_size, 2 * num_objects)\n",
    "            center_output = center_output.view(-1, self.num_objects, 2)  # Reshape to (batch_size, num_objects, 2)\n",
    "\n",
    "            # Predict class scores\n",
    "            class_output = self.fc_class(x)  # Output shape: (batch_size, num_objects * num_classes)\n",
    "            class_output = class_output.view(-1, self.num_objects, self.num_classes)  # Reshape to (batch_size, num_objects, num_classes)\n",
    "\n",
    "            out_logits.append(class_output)\n",
    "            out_center_points.append(center_output)\n",
    "\n",
    "        return {\n",
    "            'pred_logits': torch.cat(out_logits),\n",
    "            'pred_center_points': torch.cat(out_center_points)\n",
    "        }, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class SimpleCenterNetWithLSTM(nn.Module):\n",
    "#     def __init__(self, num_heads=1, num_classes=10, lstm_hidden_size=64):\n",
    "#         super(SimpleCenterNetWithLSTM, self).__init__()\n",
    "#         self.num_heads = num_heads\n",
    "#         self.num_classes = num_classes\n",
    "#         self.lstm_hidden_size = lstm_hidden_size\n",
    "\n",
    "#         # Convolutional layers\n",
    "#         self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "#         self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "#         # Fully connected layers for feature extraction\n",
    "#         self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "\n",
    "#         # LSTM layer to capture temporal dependencies\n",
    "#         self.lstm = nn.LSTM(\n",
    "#             input_size=128,\n",
    "#             hidden_size=lstm_hidden_size,\n",
    "#             num_layers=1,\n",
    "#             batch_first=False\n",
    "#         )\n",
    "\n",
    "#         self.hidden_h0 = nn.Parameter(torch.randn(1, 1, self.lstm_hidden_size))\n",
    "#         self.hidden_c0 = nn.Parameter(torch.randn(1, 1, self.lstm_hidden_size))\n",
    "\n",
    "#         # Output layers after LSTM\n",
    "#         self.fc_temporal = nn.Linear(lstm_hidden_size, 128)\n",
    "\n",
    "#         # Output layers for center points and class scores (one per head)\n",
    "#         self.fc_centers = nn.ModuleList([nn.Linear(128, 2) for _ in range(num_heads)])  # Each head predicts (x, y)\n",
    "#         self.fc_classes = nn.ModuleList([nn.Linear(128, num_classes) for _ in range(num_heads)])  # Each head predicts class scores\n",
    "\n",
    "#     def forward(self, samples, targets):\n",
    "#         samples = samples.permute(1, 0, 2, 3, 4)  # change dimension order from BT___ to TB___\n",
    "\n",
    "#         out_logits = []\n",
    "#         out_center_points = []\n",
    "\n",
    "#         # Initialize LSTM hidden state\n",
    "#         batch_size = samples.size(1)\n",
    "#         h0 = self.hidden_h0.expand(1, batch_size, self.lstm_hidden_size).contiguous()\n",
    "#         c0 = self.hidden_c0.expand(1, batch_size, self.lstm_hidden_size).contiguous()\n",
    "\n",
    "#         # Temporal feature extraction\n",
    "#         temporal_features = []\n",
    "\n",
    "#         targets = targets[0]  # We have an assumption that batch size is 1\n",
    "\n",
    "#         for timestamp, batch in enumerate(samples):\n",
    "#             keep_frame = targets[timestamp]['keep_frame'].item()\n",
    "\n",
    "#             if keep_frame:\n",
    "#                 # Forward pass through conv layers\n",
    "#                 x = F.relu(self.conv1(batch))\n",
    "#                 x = F.max_pool2d(x, 2)\n",
    "#                 x = F.relu(self.conv2(x))\n",
    "#                 x = F.max_pool2d(x, 2)\n",
    "#                 x = F.relu(self.conv3(x))\n",
    "#                 x = F.max_pool2d(x, 2)\n",
    "                \n",
    "#                 # Flatten and apply fully connected layer\n",
    "#                 x = x.view(x.size(0), -1)  # Flatten\n",
    "#                 x = F.relu(self.fc1(x))\n",
    "                \n",
    "#             else:\n",
    "#                 x = torch.zeros(1, 128).to(batch.device)\n",
    "\n",
    "#             temporal_features.append(x)\n",
    "\n",
    "#         # Stack temporal features\n",
    "#         temporal_features = torch.stack(temporal_features)\n",
    "\n",
    "#         # Pass through LSTM\n",
    "#         lstm_out, _ = self.lstm(temporal_features, (h0, c0))\n",
    "\n",
    "#         # Process LSTM output for each time step\n",
    "#         for t in range(lstm_out.size(0)):\n",
    "#             # Apply temporal feature transformation\n",
    "#             x = F.relu(self.fc_temporal(lstm_out[t]))\n",
    "\n",
    "#             # Process each head\n",
    "#             centers = []\n",
    "#             classes = []\n",
    "#             for head_idx in range(self.num_heads):\n",
    "#                 # Predict center for this head\n",
    "#                 center = self.fc_centers[head_idx](x)  # Shape: (batch_size, 2)\n",
    "#                 centers.append(center)\n",
    "\n",
    "#                 # Predict class scores for this head\n",
    "#                 class_scores = self.fc_classes[head_idx](x)  # Shape: (batch_size, num_classes)\n",
    "#                 classes.append(class_scores)\n",
    "\n",
    "#             # Stack predictions from all heads\n",
    "#             center_output = torch.stack(centers, dim=1)  # Shape: (batch_size, num_heads, 2)\n",
    "#             class_output = torch.stack(classes, dim=1)   # Shape: (batch_size, num_heads, num_classes)\n",
    "\n",
    "#             out_logits.append(class_output)\n",
    "#             out_center_points.append(center_output)\n",
    "\n",
    "#         return {\n",
    "#             'pred_logits': torch.cat(out_logits),\n",
    "#             'pred_center_points': torch.cat(out_center_points)\n",
    "#         }, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UsPDJlsKA_tJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCenterNetWithClass(nn.Module):\n",
    "    def __init__(self, num_objects=5, num_classes=10):  # num_classes for digit prediction\n",
    "        super(SimpleCenterNetWithClass, self).__init__()\n",
    "        self.num_objects = num_objects\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Fully connected layers for feature extraction\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "\n",
    "        # Output layers for center points and class scores\n",
    "        self.fc_center = nn.Linear(128, 2 * num_objects)  # Predicts (x, y) for each object\n",
    "        self.fc_class = nn.Linear(128, num_objects * num_classes)  # Predicts class scores for each object\n",
    "\n",
    "    def forward(self, samples, targets):\n",
    "\n",
    "        samples = samples.permute(1, 0, 2, 3, 4)  # change dimension order from BT___ to TB___\n",
    "\n",
    "        out_logits = []\n",
    "        out_center_points = []\n",
    "\n",
    "        for i, batch in enumerate(samples):\n",
    "          #batch = batch.permute(0, 2, 3, 1)\n",
    "\n",
    "          # Forward pass through conv layers\n",
    "          x = F.relu(self.conv1(batch))\n",
    "          x = F.max_pool2d(x, 2)\n",
    "          x = F.relu(self.conv2(x))\n",
    "          x = F.max_pool2d(x, 2)\n",
    "          x = F.relu(self.conv3(x))\n",
    "          x = F.max_pool2d(x, 2)\n",
    "\n",
    "          # Flatten and apply fully connected layer\n",
    "          x = x.view(x.size(0), -1)  # Flatten\n",
    "          x = F.relu(self.fc1(x))\n",
    "\n",
    "          # Predict centers\n",
    "          center_output = self.fc_center(x)  # Output shape: (batch_size, 2 * num_objects)\n",
    "          center_output = center_output.view(-1, self.num_objects, 2)  # Reshape to (batch_size, num_objects, 2)\n",
    "\n",
    "          # Predict class scores\n",
    "          class_output = self.fc_class(x)  # Output shape: (batch_size, num_objects * num_classes)\n",
    "          class_output = class_output.view(-1, self.num_objects, self.num_classes)  # Reshape to (batch_size, num_objects, num_classes)\n",
    "\n",
    "          out_logits.append(class_output)\n",
    "          out_center_points.append(center_output)\n",
    "\n",
    "        return {\n",
    "            'pred_logits': torch.cat(out_logits),\n",
    "            'pred_center_points': torch.cat(out_center_points)\n",
    "        }, targets[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2021 Phil Wang\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "# Modifications copyright (C) 2024 Maksim Ploter\n",
    "\n",
    "from functools import wraps\n",
    "from math import pi\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Reduce\n",
    "from torch import nn, einsum\n",
    "\n",
    "\n",
    "# helpers\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "\n",
    "def cache_fn(f):\n",
    "    cache = dict()\n",
    "\n",
    "    @wraps(f)\n",
    "    def cached_fn(*args, _cache=True, key=None, **kwargs):\n",
    "        if not _cache:\n",
    "            return f(*args, **kwargs)\n",
    "        nonlocal cache\n",
    "        if key in cache:\n",
    "            return cache[key]\n",
    "        result = f(*args, **kwargs)\n",
    "        cache[key] = result\n",
    "        return result\n",
    "\n",
    "    return cached_fn\n",
    "\n",
    "\n",
    "def fourier_encode(x, max_freq, num_bands=4):\n",
    "    x = x.unsqueeze(-1)\n",
    "    device, dtype, orig_x = x.device, x.dtype, x\n",
    "\n",
    "    scales = torch.linspace(1., max_freq / 2, num_bands, device=device, dtype=dtype)\n",
    "    scales = scales[(*((None,) * (len(x.shape) - 1)), Ellipsis)]\n",
    "\n",
    "    x = x * scales * pi\n",
    "    x = torch.cat([x.sin(), x.cos()], dim=-1)\n",
    "    x = torch.cat((x, orig_x), dim=-1)\n",
    "    return x\n",
    "\n",
    "\n",
    "# helper classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn, context_dim=None):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.norm_context = nn.LayerNorm(context_dim) if exists(context_dim) else None\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if exists(self.norm_context):\n",
    "            context = kwargs['context']\n",
    "            normed_context = self.norm_context(context)\n",
    "            kwargs.update(context=normed_context)\n",
    "\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "\n",
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gates = x.chunk(2, dim=-1)\n",
    "        return x * F.gelu(gates)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult=4, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult * 2),\n",
    "            GEGLU(),\n",
    "            nn.Linear(dim * mult, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        context_dim = default(context_dim, query_dim)\n",
    "\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "\n",
    "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
    "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.to_out = nn.Linear(inner_dim, query_dim)\n",
    "\n",
    "    def forward(self, x, context=None, mask=None):\n",
    "        h = self.heads\n",
    "\n",
    "        q = self.to_q(x)\n",
    "        context = default(context, x)\n",
    "        k, v = self.to_kv(context).chunk(2, dim=-1)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n",
    "\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b ... -> b (...)')\n",
    "            max_neg_value = -torch.finfo(sim.dtype).max\n",
    "            mask = repeat(mask, 'b j -> (b h) () j', h=h)\n",
    "            sim.masked_fill_(mask, max_neg_value)  # Fills elements of self tensor with value where mask is True\n",
    "\n",
    "        # attention, what we cannot get enough of\n",
    "        attn = sim.softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "# main class\n",
    "\n",
    "class Perceiver(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *,\n",
    "            num_freq_bands,\n",
    "            depth,\n",
    "            max_freq,\n",
    "            input_channels=3,\n",
    "            input_axis=2,\n",
    "            num_latents=512,\n",
    "            latent_dim=512,\n",
    "            cross_heads=1,\n",
    "            latent_heads=8,\n",
    "            cross_dim_head=64,\n",
    "            latent_dim_head=64,\n",
    "            num_classes=1000,\n",
    "            attn_dropout=0.,\n",
    "            ff_dropout=0.,\n",
    "            weight_tie_layers=False,\n",
    "            fourier_encode_data=True,\n",
    "            self_per_cross_attn=1,\n",
    "            final_classifier_head=True\n",
    "    ):\n",
    "        \"\"\"The shape of the final attention mechanism will be:\n",
    "        depth * (cross attention -> self_per_cross_attn * self attention)\n",
    "\n",
    "        Args:\n",
    "          num_freq_bands: Number of freq bands, with original value (2 * K + 1)\n",
    "          depth: Depth of net.\n",
    "          max_freq: Maximum frequency, hyperparameter depending on how\n",
    "              fine the data is.\n",
    "          freq_base: Base for the frequency\n",
    "          input_channels: Number of channels for each token of the input.\n",
    "          input_axis: Number of axes for input data (2 for images, 3 for video)\n",
    "          num_latents: Number of latents, or induced set points, or centroids.\n",
    "              Different papers giving it different names.\n",
    "          latent_dim: Latent dimension.\n",
    "          cross_heads: Number of heads for cross attention. Paper said 1.\n",
    "          latent_heads: Number of heads for latent self attention, 8.\n",
    "          cross_dim_head: Number of dimensions per cross attention head.\n",
    "          latent_dim_head: Number of dimensions per latent self attention head.\n",
    "          num_classes: Output number of classes.\n",
    "          attn_dropout: Attention dropout\n",
    "          ff_dropout: Feedforward dropout\n",
    "          weight_tie_layers: Whether to weight tie layers (optional).\n",
    "          fourier_encode_data: Whether to auto-fourier encode the data, using\n",
    "              the input_axis given. defaults to True, but can be turned off\n",
    "              if you are fourier encoding the data yourself.\n",
    "          self_per_cross_attn: Number of self attention blocks per cross attn.\n",
    "          final_classifier_head: mean pool and project embeddings to number of classes (num_classes) at the end\n",
    "        \"\"\"\n",
    "        for param_name, param_value in locals().items():\n",
    "            if param_name != 'self':\n",
    "                print(f\"{param_name}: {param_value}\")\n",
    "        super().__init__()\n",
    "        self.input_axis = input_axis\n",
    "        self.max_freq = max_freq\n",
    "        self.num_freq_bands = num_freq_bands\n",
    "\n",
    "        self.fourier_encode_data = fourier_encode_data\n",
    "        fourier_channels = (input_axis * ((num_freq_bands * 2) + 1)) if fourier_encode_data else 0\n",
    "        input_dim = fourier_channels + input_channels\n",
    "\n",
    "        self.latents = nn.Parameter(torch.randn(num_latents, latent_dim))\n",
    "\n",
    "        get_cross_attn = lambda: PreNorm(latent_dim,\n",
    "                                         Attention(latent_dim, input_dim, heads=cross_heads, dim_head=cross_dim_head,\n",
    "                                                   dropout=attn_dropout), context_dim=input_dim)\n",
    "        get_cross_ff = lambda: PreNorm(latent_dim, FeedForward(latent_dim, dropout=ff_dropout))\n",
    "        get_latent_attn = lambda: PreNorm(latent_dim,\n",
    "                                          Attention(latent_dim, heads=latent_heads, dim_head=latent_dim_head,\n",
    "                                                    dropout=attn_dropout))\n",
    "        get_latent_ff = lambda: PreNorm(latent_dim, FeedForward(latent_dim, dropout=ff_dropout))\n",
    "\n",
    "        get_cross_attn, get_cross_ff, get_latent_attn, get_latent_ff = map(cache_fn, (\n",
    "        get_cross_attn, get_cross_ff, get_latent_attn, get_latent_ff))\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for i in range(depth):\n",
    "            should_cache = i > 0 and weight_tie_layers\n",
    "            cache_args = {'_cache': should_cache}\n",
    "\n",
    "            self_attns = nn.ModuleList([])\n",
    "\n",
    "            for block_ind in range(self_per_cross_attn):\n",
    "                self_attns.append(nn.ModuleList([\n",
    "                    get_latent_attn(**cache_args, key=block_ind),\n",
    "                    get_latent_ff(**cache_args, key=block_ind)\n",
    "                ]))\n",
    "\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                get_cross_attn(**cache_args),\n",
    "                get_cross_ff(**cache_args),\n",
    "                self_attns\n",
    "            ]))\n",
    "\n",
    "        self.to_logits = nn.Sequential(\n",
    "            Reduce('b n d -> b d', 'mean'),\n",
    "            nn.LayerNorm(latent_dim),\n",
    "            nn.Linear(latent_dim, num_classes)\n",
    "        ) if final_classifier_head else nn.Identity()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            data,  # b ()\n",
    "            latents=None,  # (b, num_latents, latent_dim)\n",
    "            mask=None,\n",
    "            return_embeddings=False\n",
    "    ):\n",
    "        b, *axis, _, device, dtype = *data.shape, data.device, data.dtype\n",
    "        assert len(axis) == self.input_axis, 'input data must have the right number of axis'\n",
    "\n",
    "        if self.fourier_encode_data:\n",
    "            # calculate fourier encoded positions in the range of [-1, 1], for all axis\n",
    "\n",
    "            axis_pos = list(map(lambda size: torch.linspace(-1., 1., steps=size, device=device, dtype=dtype), axis))\n",
    "            pos = torch.stack(torch.meshgrid(*axis_pos, indexing='ij'), dim=-1)\n",
    "            enc_pos = fourier_encode(pos, self.max_freq, self.num_freq_bands)\n",
    "            enc_pos = rearrange(enc_pos, '... n d -> ... (n d)')\n",
    "            enc_pos = repeat(enc_pos, '... -> b ...', b=b)\n",
    "\n",
    "            data = torch.cat((data, enc_pos), dim=-1)\n",
    "\n",
    "        # concat to channels of data and flatten axis\n",
    "\n",
    "        data = rearrange(data, 'b ... d -> b (...) d')\n",
    "\n",
    "        if latents is not None:\n",
    "            x = latents\n",
    "        else:\n",
    "            x = repeat(self.latents, 'n d -> b n d', b=b)\n",
    "\n",
    "        # layers\n",
    "\n",
    "        for cross_attn, cross_ff, self_attns in self.layers:\n",
    "            x_cross = cross_attn(x, context=data, mask=mask)\n",
    "\n",
    "            if mask is not None:\n",
    "                # Check which rows (batch-wise) have all elements equal to 1 (fully masked)\n",
    "                # mask_all_ones will be a [batch_size] tensor of True/False\n",
    "                mask_all_ones = mask.view(mask.size(0), -1).all(dim=1)\n",
    "\n",
    "                mask_all_ones = mask_all_ones.view(-1, 1, 1)\n",
    "\n",
    "                # Zero out the corresponding rows in tgt2\n",
    "                x_cross = x_cross.masked_fill(mask_all_ones, float(0))\n",
    "\n",
    "            x = x_cross + x\n",
    "\n",
    "            x = cross_ff(x) + x\n",
    "\n",
    "            for self_attn, self_ff in self_attns:\n",
    "                x = self_attn(x) + x\n",
    "                x = self_ff(x) + x\n",
    "\n",
    "        # allow for fetching embeddings\n",
    "\n",
    "        if return_embeddings:\n",
    "            return x\n",
    "\n",
    "        # to logits\n",
    "\n",
    "        return self.to_logits(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceiver Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "\n",
    "class PerceiverDetection(nn.Module):\n",
    "\n",
    "    def __init__(self, backbone, perceiver, classification_head):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.perceiver = perceiver\n",
    "        self.classification_head = classification_head\n",
    "        # Compatibility with TrackingBaseModel\n",
    "        self.num_queries = perceiver.latents.shape[0]\n",
    "        self.hidden_dim = perceiver.latents.shape[1]\n",
    "        self.overflow_boxes = False\n",
    "\n",
    "    def forward(self, samples, targets: list = None, latents: Tensor = None):\n",
    "\n",
    "        src = self.backbone(samples)\n",
    "        src = src.permute(0, 2, 3, 1)\n",
    "        \n",
    "        hs = self.perceiver(\n",
    "            data=src,\n",
    "            return_embeddings=True,\n",
    "            latents=latents\n",
    "        )\n",
    "        out = self.classification_head(hs)\n",
    "\n",
    "        # TODO: double check if normilization should be disabled\n",
    "        out['hs_embed'] = hs\n",
    "\n",
    "        return (\n",
    "            out,\n",
    "            targets,\n",
    "            None,\n",
    "            None,  # Memory, is an output from encoder\n",
    "            hs\n",
    "        )\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ObjectDetectionHead(nn.Module):\n",
    "    def __init__(self, num_classes, num_latents, latent_dim):\n",
    "        \"\"\" Initializes the model.\n",
    "        Parameters:\n",
    "            num_classes: number of object classes\n",
    "            num_latents: number of object queries, ie detection slot. This is the maximal number of objects\n",
    "                         model can detect in a single image. For COCO, we recommend 100 queries.\n",
    "            latent_dim: dimension of the latent object query.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_queries = num_latents\n",
    "        self.class_embed = nn.Linear(latent_dim, num_classes + 1)\n",
    "        self.center_points_embed = MLP(latent_dim, latent_dim, 2, 3)\n",
    "\n",
    "    def forward(self, hs: Tensor):\n",
    "        \"\"\"Forward pass of the ObjectDetectionHead.\n",
    "            Parameters:\n",
    "                - hs: Tensor\n",
    "                    Hidden states from the model, of shape [batch_size x num_queries x latent_dim].\n",
    "\n",
    "            It returns a dict with the following elements:\n",
    "               - \"pred_logits\": the classification logits (including no-object) for all queries.\n",
    "                                Shape= [batch_size x num_queries x (num_classes + 1)]\n",
    "               - \"pred_boxes\": The normalized boxes coordinates for all queries, represented as\n",
    "                               (center_x, center_y, height, width). These values are normalized in [0, 1],\n",
    "                               relative to the size of each individual image (disregarding possible padding).\n",
    "                               See PostProcess for information on how to retrieve the unnormalized bounding box.\n",
    "        \"\"\"\n",
    "        outputs_class = self.class_embed(hs)\n",
    "        outputs_coord = self.center_points_embed(hs).sigmoid()\n",
    "        out = {'pred_logits': outputs_class, 'pred_center_points': outputs_coord}\n",
    "        return out\n",
    "\n",
    "\n",
    "def build_model_perceiver(args, num_classes):\n",
    "\n",
    "    device = torch.device(args.device)\n",
    "\n",
    "    backbone = nn.Identity()\n",
    "\n",
    "    num_freq_bands = args.num_freq_bands\n",
    "    fourier_channels = 2 * ((num_freq_bands * 2) + 1)\n",
    "\n",
    "    num_queries = args.num_objects\n",
    "    num_channels = 1 # no backbone, image is gray scale\n",
    "    \n",
    "    perceiver = Perceiver(\n",
    "        input_channels=num_channels,  # number of channels for each token of the input\n",
    "        input_axis=2,  # number of axis for input data (2 for images, 3 for video)\n",
    "        num_freq_bands=num_freq_bands,  # number of freq bands, with original value (2 * K + 1)\n",
    "        max_freq=args.max_freq,  # maximum frequency, hyperparameter depending on how fine the data is\n",
    "        depth=args.enc_layers,  # depth of net. The shape of the final attention mechanism will be:\n",
    "        #   depth * (cross attention -> self_per_cross_attn * self attention)\n",
    "        num_latents=num_queries,\n",
    "        # number of latents, or induced set points, or centroids. different papers giving it different names\n",
    "        latent_dim=args.hidden_dim,  # latent dimension\n",
    "        cross_heads=args.enc_nheads_cross,  # number of heads for cross attention. paper said 1\n",
    "        latent_heads=args.nheads,  # number of heads for latent self attention, 8\n",
    "        cross_dim_head=(num_channels + fourier_channels) // args.enc_nheads_cross,\n",
    "        # number of dimensions per cross attention head\n",
    "        latent_dim_head=args.hidden_dim // args.nheads,  # number of dimensions per latent self attention head\n",
    "        num_classes=-1,  # NOT USED. output number of classes.\n",
    "        attn_dropout=args.dropout,\n",
    "        ff_dropout=args.dropout,\n",
    "        weight_tie_layers=False,  # whether to weight tie layers (optional, as indicated in the diagram)\n",
    "        fourier_encode_data=True,\n",
    "        # whether to auto-fourier encode the data, using the input_axis given. defaults to True, but can be turned off if you are fourier encoding the data yourself\n",
    "        self_per_cross_attn=args.self_per_cross_attn,  # number of self attention blocks per cross attention\n",
    "        final_classifier_head=False  # mean pool and project embeddings to number of classes (num_classes) at the end\n",
    "    )\n",
    "\n",
    "    classifier_head = ObjectDetectionHead(\n",
    "        num_classes=num_classes,\n",
    "        num_latents=num_queries,\n",
    "        latent_dim=args.hidden_dim\n",
    "    )\n",
    "\n",
    "    return backbone, perceiver, classifier_head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceiver AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverAr(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 detection_model,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.detection_model = detection_model\n",
    "\n",
    "    def forward(self, samples, targets: list = None):\n",
    "        if len(samples.shape) < 5:\n",
    "            # samples without a time dimension\n",
    "            raise NotImplementedError(\"Not implemented yet samples without a time dimension.\")\n",
    "\n",
    "        src = samples.permute(1, 0, 2, 3, 4)  # change dimension order from BT___ to TB___\n",
    "\n",
    "        device = src.device\n",
    "        result = {'pred_logits': [], 'pred_center_points': []}\n",
    "\n",
    "        orig_size = torch.stack([t[-1][\"orig_size\"] for t in targets], dim=0).to(device)\n",
    "\n",
    "        out_baseline = None\n",
    "        hs = None\n",
    "\n",
    "        assert len(targets) == 1\n",
    "        targets = targets[0] # We have an assumption that batch size is 1\n",
    "\n",
    "        for timestamp, batch in enumerate(src):\n",
    "            keep_frame = targets[timestamp]['keep_frame'].item()\n",
    "\n",
    "            if not keep_frame:\n",
    "                # drop the frame\n",
    "                batch = torch.zeros_like(batch)\n",
    "\n",
    "            out, targets_resp, features, memory, hs = self.detection_model.forward(\n",
    "                samples=batch, targets=None, latents=hs\n",
    "            )\n",
    "\n",
    "            result['pred_logits'].extend(out['pred_logits'])\n",
    "            result['pred_center_points'].extend(out['pred_center_points'])\n",
    "\n",
    "        return {\n",
    "            'pred_logits': torch.stack(result['pred_logits']),\n",
    "            'pred_center_points': torch.stack(result['pred_center_points'])\n",
    "        }, targets\n",
    "\n",
    "def build_perceiver_ar_model(args, num_classes):\n",
    "    backbone, perceiver, classification_head = build_model_perceiver(args, num_classes=num_classes)\n",
    "    \n",
    "    detection_model = PerceiverDetection(\n",
    "        backbone, perceiver, classification_head    \n",
    "    ) \n",
    "    model = PerceiverAr(\n",
    "        detection_model = detection_model\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "5LXWR15lBElp"
   },
   "outputs": [],
   "source": [
    "def build_model(args):\n",
    "    assert 'moving-mnist' in args.dataset.lower()\n",
    "    num_classes = 10\n",
    "    dataset_name = args.dataset.lower()\n",
    "    if '1digit' in dataset_name:\n",
    "        num_objects = 1\n",
    "    elif '2digit' in dataset_name:\n",
    "        num_objects = 2\n",
    "    else:\n",
    "        raise ValueError(f'unknown {dataset_name}')\n",
    "    \n",
    "    if args.model == 'lstm':\n",
    "        return SimpleCenterNetWithLSTM(num_objects=args.num_objects, num_classes=num_classes, lstm_hidden_size=args.lstm_hidden_size)\n",
    "    elif args.model == 'cnn':\n",
    "        return SimpleCenterNetWithClass(\n",
    "            num_heads=args.num_objects, \n",
    "            num_classes=num_classes\n",
    "        )\n",
    "    elif args.model == 'perceiver':\n",
    "        return build_perceiver_ar_model(args, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oESlZdhU_X2R"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wKThxzNihWKu"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "def get_affine_transformed_coordinates(point, center, angle=0, translate=(0, 0), scale=1, shear=(0, 0)):\n",
    "    # Convert degrees to radians for rotation and shear\n",
    "    x0, y0 = point\n",
    "    theta = math.radians(angle)\n",
    "    shear_x = math.radians(shear[0]) if isinstance(shear, (list, tuple)) else math.radians(shear)\n",
    "    shear_y = math.radians(shear[1]) if isinstance(shear, (list, tuple)) else 0.0\n",
    "\n",
    "    # Calculate the affine transformation matrix components\n",
    "    a = scale * math.cos(theta + shear_x)\n",
    "    b = -scale * math.sin(theta + shear_y)\n",
    "    c = scale * math.sin(theta + shear_x)\n",
    "    d = scale * math.cos(theta + shear_y)\n",
    "\n",
    "    # Adjust translation to keep the transformation centered\n",
    "    tx, ty = translate\n",
    "    cx, cy = center\n",
    "    tx = tx + cx - (a * cx + b * cy)\n",
    "    ty = ty + cy - (c * cx + d * cy)\n",
    "\n",
    "    # Apply the affine transformation\n",
    "    x1 = a * x0 + b * y0 + tx\n",
    "    y1 = c * x0 + d * y0 + ty\n",
    "\n",
    "    return x1, y1\n",
    "\n",
    "assert get_affine_transformed_coordinates(point=(32, 32), angle=0, translate=(10, 10), scale=1, shear=(0, 0), center=(32,32)) == (42.0, 42.0)\n",
    "assert get_affine_transformed_coordinates(point=(32, 32), angle=0, translate=(-2, 2), scale=1, shear=(0, 0), center=(32,32)) == (30, 34)\n",
    "assert get_affine_transformed_coordinates(point=(32, 32), angle=30, translate=(10, 10), scale=1, shear=(0, 0), center=(32,32)) == (42.0, 42.0)\n",
    "assert get_affine_transformed_coordinates(point=(32, 32), angle=30, translate=(10, 10), scale=2, shear=(0, 0), center=(32,32)) == (42.0, 42.0)\n",
    "assert (p:=get_affine_transformed_coordinates(point=(10, 10), angle=30, translate=(0, 0), scale=1, shear=(0, 0), center=(32,32))) == (23.947441116742347, 1.9474411167423504), f'Output: {p}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "id": "hv07T4SfEE5J",
    "outputId": "ad8e3c37-54e1-488e-d85f-9f8a32b16829"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Point Coordinates: (56.430780618346944, 29.751288694035736)\n",
      "Pixel Value at Manual Point: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: Original point and affine parameters\n",
    "image = torch.zeros(1, 1, 128, 128)  # Blank image for simplicity\n",
    "original_point = (40, 50)            # The point to track\n",
    "image[:, :, original_point[1], original_point[0]] = 1  # Mark the point on the image\n",
    "\n",
    "# Transformation parameters\n",
    "center = (64, 64)  # Center of transformation\n",
    "angle = 30         # Rotation angle in degrees\n",
    "translate = (20, 14)  # Translation in x and y\n",
    "scale = 2         # Scaling factor\n",
    "shear = (0, 0)     # Shear angles\n",
    "\n",
    "# Step 1: Transform the image using TF.affine\n",
    "transformed_image = TF.affine(image, angle=angle, translate=translate, scale=scale, shear=shear)\n",
    "\n",
    "# Step 2: Manually calculate the transformed coordinates\n",
    "manual_point = get_affine_transformed_coordinates(\n",
    "    point=original_point,\n",
    "    center=center,\n",
    "    angle=angle,\n",
    "    translate=translate,\n",
    "    scale=scale,\n",
    "    shear=shear\n",
    ")\n",
    "\n",
    "# Step 3: Locate the pixel at the manually transformed position\n",
    "manual_x, manual_y = map(int, manual_point)\n",
    "if 0 <= manual_x < transformed_image.shape[3] and 0 <= manual_y < transformed_image.shape[2]:\n",
    "    manual_pixel_value = transformed_image[0, 0, manual_y, manual_x].item()\n",
    "else:\n",
    "    manual_pixel_value = None  # Out-of-bounds case\n",
    "\n",
    "# Step 4: Visualize and compare\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Original image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Image with Point\")\n",
    "plt.imshow(image.squeeze(0).squeeze(0), cmap=\"gray\")\n",
    "plt.scatter([original_point[0]], [original_point[1]], color=\"red\", label=\"Original Point\")\n",
    "plt.legend()\n",
    "\n",
    "# Transformed image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Transformed Image with Manual Point\")\n",
    "plt.imshow(transformed_image.squeeze(0).squeeze(0), cmap=\"gray\")\n",
    "plt.scatter([manual_x], [manual_y], color=\"blue\", label=\"Manual Transformed Point\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Compare the manual and ground truth\n",
    "print(f\"Manual Point Coordinates: {manual_point}\")\n",
    "print(f\"Pixel Value at Manual Point: {manual_pixel_value}\")\n",
    "\n",
    "# Optional: Calculate Euclidean distance between manual and observed position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.12 (main, Apr  5 2022, 06:56:58) \n",
      "[GCC 7.5.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "I8kRHVly4v_9"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the directory containing the module to the Python path\n",
    "sys.path.append('/gpfs/helios/home/ploter/projects/MovingMNIST/torch_moving_mnist')\n",
    "\n",
    "from functools import partial\n",
    "from types import SimpleNamespace\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "# %% ../nbs/01_data.ipynb 10\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from torch_moving_mnist.data import *\n",
    "\n",
    "# %% ../nbs/01_data.ipynb 34\n",
    "import math\n",
    "import random\n",
    "from fastprogress import progress_bar\n",
    "\n",
    "import copy\n",
    "\n",
    "# def apply_n_times(tf, tcf, x, y, n=1):\n",
    "#     sequence = [x]\n",
    "#     sequence_y = [(round(y[0]), round(y[1]))]\n",
    "\n",
    "#     for n in range(n):\n",
    "#         sequence.append(tf(sequence[n]))\n",
    "\n",
    "#         x1, y1 = tcf(sequence_y[n])\n",
    "#         sequence_y.append(\n",
    "#             (round(x1), round(y1))\n",
    "#         )\n",
    "\n",
    "#     return sequence, sequence_y\n",
    "\n",
    "# class RandomTrajectory:\n",
    "#     def __init__(self, affine_params, center, n=5, **kwargs):\n",
    "#         self.angle     = random.uniform(*affine_params.angle)\n",
    "#         self.translate = (random.uniform(*affine_params.translate[0]),\n",
    "#                           random.uniform(*affine_params.translate[1]))\n",
    "#         self.scale     = random.uniform(*affine_params.scale)\n",
    "#         self.shear     = random.uniform(*affine_params.shear)\n",
    "#         self.n = n\n",
    "#         self.center = center\n",
    "#         self.tf = partial(TF.affine, angle=self.angle, translate=self.translate, scale=self.scale, shear=self.shear, **kwargs)\n",
    "#         self.tcf = partial(\n",
    "#             get_affine_transformed_coordinates,\n",
    "#             angle=self.angle,\n",
    "#             translate=self.translate,\n",
    "#             scale=self.scale,\n",
    "#             shear=self.shear,\n",
    "#             center=self.center\n",
    "#         )\n",
    "\n",
    "#     def __call__(self, img, target):\n",
    "#         return apply_n_times(self.tf, self.tcf, img, target, n=self.n)\n",
    "\n",
    "#     def __repr__(self):\n",
    "#         s = (\"RandomTrajectory(\\n\"\n",
    "#              f\"  angle:     {self.angle}\\n\"\n",
    "#              f\"  translate: {self.translate}\\n\"\n",
    "#              f\"  scale:     {self.scale}\\n\"\n",
    "#              f\"  shear:     {self.shear}\\n)\")\n",
    "#         return s\n",
    "\n",
    "def check_boundary_collision(point, next_point, img_size, digit_size=28, use_center=True):\n",
    "    \"\"\"\n",
    "    Check if digit hits boundary or would cross it in next step.\n",
    "    Returns collision flags and adjusted next position.\n",
    "    Args:\n",
    "        point: Current (x, y) coordinates\n",
    "        next_point: Next predicted (x, y) coordinates\n",
    "        img_size: Size of the frame\n",
    "        digit_size: Size of the MNIST digit (default 28)\n",
    "        use_center: If True, bounce when center hits boundary. If False, bounce when digit boundary hits\n",
    "    \"\"\"\n",
    "    x, y = point\n",
    "    next_x, next_y = next_point\n",
    "    half_digit = digit_size // 2\n",
    "    # Determine boundaries based on configuration\n",
    "    if use_center:\n",
    "        min_bound = 0\n",
    "        max_bound = img_size\n",
    "    else:\n",
    "        min_bound = half_digit\n",
    "        max_bound = img_size - half_digit\n",
    "    # Check if next position would cross boundary and adjust it\n",
    "    if next_x <= min_bound:\n",
    "        next_x = min_bound + (min_bound - next_x)\n",
    "        collision_x = True\n",
    "    elif next_x >= max_bound:\n",
    "        next_x = max_bound - (next_x - max_bound)\n",
    "        collision_x = True\n",
    "    else:\n",
    "        collision_x = False\n",
    "    if next_y <= min_bound:\n",
    "        next_y = min_bound + (min_bound - next_y)\n",
    "        collision_y = True\n",
    "    elif next_y >= max_bound:\n",
    "        next_y = max_bound - (next_y - max_bound)\n",
    "        collision_y = True\n",
    "    else:\n",
    "        collision_y = False\n",
    "    return collision_x, collision_y, (next_x, next_y)\n",
    "\n",
    "def reflect_trajectory(translate, collision_x, collision_y):\n",
    "    \"\"\"Reflect trajectory based on collision\"\"\"\n",
    "    tx, ty = translate\n",
    "    if collision_x:\n",
    "        tx = -tx\n",
    "    if collision_y:\n",
    "        ty = -ty\n",
    "    return (tx, ty)\n",
    "\n",
    "class RandomTrajectory:\n",
    "    def __init__(self, affine_params, center, n=5, bounce=True, use_center_bounce=True, **kwargs):\n",
    "        self.angle = random.uniform(*affine_params.angle)\n",
    "        self.translate = (random.uniform(*affine_params.translate[0]),\n",
    "                         random.uniform(*affine_params.translate[1]))\n",
    "        self.scale = random.uniform(*affine_params.scale)\n",
    "        self.shear = random.uniform(*affine_params.shear)\n",
    "        self.n = n\n",
    "        self.center = center\n",
    "        self.bounce = bounce\n",
    "        self.use_center_bounce = use_center_bounce\n",
    "        self.img_size = center[0] * 2  # Assuming square image\n",
    "        self.tf = partial(TF.affine, angle=self.angle, translate=self.translate, \n",
    "                         scale=self.scale, shear=self.shear, **kwargs)\n",
    "        self.tcf = partial(\n",
    "            get_affine_transformed_coordinates,\n",
    "            angle=self.angle,\n",
    "            translate=self.translate,\n",
    "            scale=self.scale,\n",
    "            shear=self.shear,\n",
    "            center=self.center\n",
    "        )\n",
    "    def __call__(self, img, target):\n",
    "        sequence = [img]\n",
    "        sequence_y = [(round(target[0]), round(target[1]))]\n",
    "        current_translate = self.translate\n",
    "        for _ in range(self.n):\n",
    "            # Calculate next position before applying transformation\n",
    "            if self.bounce:\n",
    "                # Get next position without actually moving\n",
    "                next_tcf = partial(\n",
    "                    get_affine_transformed_coordinates,\n",
    "                    angle=self.angle,\n",
    "                    translate=current_translate,\n",
    "                    scale=self.scale,\n",
    "                    shear=self.shear,\n",
    "                    center=self.center\n",
    "                )\n",
    "                next_x, next_y = next_tcf(sequence_y[-1])\n",
    "                # Check for boundary collision and get adjusted position\n",
    "                collision_x, collision_y, adjusted_next_pos = check_boundary_collision(\n",
    "                    sequence_y[-1],\n",
    "                    (next_x, next_y),\n",
    "                    self.img_size,\n",
    "                    use_center=self.use_center_bounce\n",
    "                )\n",
    "                if collision_x or collision_y:\n",
    "                    # Update translation for next frame\n",
    "                    current_translate = reflect_trajectory(\n",
    "                        current_translate, collision_x, collision_y\n",
    "                    )\n",
    "                    # Calculate transformation to reach adjusted position\n",
    "                    dx = adjusted_next_pos[0] - sequence_y[-1][0]\n",
    "                    dy = adjusted_next_pos[1] - sequence_y[-1][1]\n",
    "                    current_translate = (dx, dy)\n",
    "            # Apply transformation with potentially adjusted translation\n",
    "            tf = partial(TF.affine, angle=self.angle, translate=current_translate,\n",
    "                        scale=self.scale, shear=self.shear)\n",
    "            tcf = partial(\n",
    "                get_affine_transformed_coordinates,\n",
    "                angle=self.angle,\n",
    "                translate=current_translate,\n",
    "                scale=self.scale,\n",
    "                shear=self.shear,\n",
    "                center=self.center\n",
    "            )\n",
    "            sequence.append(tf(sequence[-1]))\n",
    "            x1, y1 = tcf(sequence_y[-1])\n",
    "            sequence_y.append((round(x1), round(y1)))\n",
    "        return sequence, sequence_y\n",
    "\n",
    "\n",
    "class MovingMNIST(Dataset):\n",
    "    def __init__(self, path=\".\",  # path to store the MNIST dataset\n",
    "                 affine_params: dict=affine_params, # affine transform parameters, refer to torchvision.transforms.functional.affine\n",
    "                 num_digits: list[int]=[1,2], # how many digits to move, random choice between the value provided\n",
    "                 num_frames: int=4, # how many frames to create\n",
    "                 img_size=64, # the canvas size, the actual digits are always 28x28\n",
    "                 concat=True, # if we concat the final results (frames, 1, 28, 28) or a list of frames.\n",
    "                 normalize=False, # scale images in [0,1] and normalize them with MNIST stats. Applied at batch level. Have to take care of the canvas size that messes up the stats!\n",
    "                 bounce=True,  # Enable/disable bouncing\n",
    "                 use_center_bounce=True,  # Use center or boundary for bounce detection\n",
    "                 frame_dropout_pattern = None,\n",
    "                 sequences_path = None, #TODO: REMOVE\n",
    "                 split_indices=None,\n",
    "                 sampler_steps=[], # epochs at which assign coresponding frame dropout probability\n",
    "                 frame_dropout_probs=[], # absolut frame drop probability values\n",
    "                 ):\n",
    "        self.bounce = bounce\n",
    "        self.use_center_bounce = use_center_bounce\n",
    "        self.sequences = None\n",
    "        if sequences_path is not None:\n",
    "          data = torch.load(sequences_path)\n",
    "          self.sequences = list(zip(data['imgs'], data['targets']))\n",
    "\n",
    "          num_frames = self.sequences[0][0].shape[0]\n",
    "          img_size = self.sequences[0][0].shape[2]\n",
    "          print(f'Num frames: {num_frames}')\n",
    "          print(f'Img size: {img_size}')\n",
    "\n",
    "        self.num_digits = num_digits\n",
    "        if self.sequences is None:\n",
    "          mnist = MNIST(path, download=True)\n",
    "          self.mnist_dataset = mnist.data\n",
    "          self.mnist_targets = mnist.targets\n",
    "\n",
    "          if split_indices is not None:\n",
    "            self.mnist_dataset = self.mnist_dataset[split_indices]\n",
    "            self.mnist_targets = self.mnist_targets[split_indices]\n",
    "          self.ids = [[random.randrange(0, len(self.mnist_dataset)) for _ in range(random.choice(self.num_digits))] for _ in range(len(self.mnist_dataset))]\n",
    "          self.affine_params = affine_params\n",
    "\n",
    "        self.num_frames = num_frames\n",
    "        self.img_size = img_size\n",
    "        self.pad = padding(img_size)\n",
    "        self.concat = concat\n",
    "\n",
    "        self.keep_frame_mask = None\n",
    "        self.frame_dropout_prob = 0.0\n",
    "        \n",
    "        self.sampler_steps = sampler_steps\n",
    "        self.frame_dropout_probs = frame_dropout_probs\n",
    "        print(\"sampler_steps={} frame_dropout_probs={}\".format(self.sampler_steps, self.frame_dropout_probs))\n",
    "        \n",
    "        if self.sampler_steps is not None and len(self.sampler_steps) > 0:\n",
    "            # Enable sampling length adjustment.\n",
    "            assert len(self.frame_dropout_probs) > 0\n",
    "            assert len(self.frame_dropout_probs) == len(self.sampler_steps) + 1\n",
    "            for i in range(len(self.sampler_steps) - 1):\n",
    "                assert self.sampler_steps[i] < self.sampler_steps[i + 1]\n",
    "            self.period_idx = 0\n",
    "            self.frame_dropout_prob = self.frame_dropout_probs[0]\n",
    "            self.current_epoch = 0\n",
    "\n",
    "        if frame_dropout_pattern is not None:\n",
    "          print('Disable probability based frame drops. Use frame drops based on fixed mask.')\n",
    "          self.frame_dropout_prob = None\n",
    "          self.sampler_steps = []\n",
    "          drop_frame_mask = torch.tensor([int(char) for char in frame_dropout_pattern])\n",
    "          self.keep_frame_mask = 1 - drop_frame_mask\n",
    "          assert self.keep_frame_mask.size(0) == self.num_frames, f\"Frame dropout pattern must have the same length as the number of frames. Num of frames {self.num_frames} and mask size {self.keep_frame_mask.size(0)}\"\n",
    "          print(f'Set frame keep mask: {self.keep_frame_mask}')\n",
    "\n",
    "        # some computation to ensure normalizing correctly-ish\n",
    "        batch_tfms = [T.ConvertImageDtype(torch.float32)]\n",
    "        if normalize:\n",
    "            ratio = (28/img_size)**2*max(num_digits)\n",
    "            mean, std = mnist_stats\n",
    "            scaled_mnist_stats = ([mean[0]*ratio], [std[0]*ratio])\n",
    "            print(f\"New computed stats for MovingMNIST: {scaled_mnist_stats}\")\n",
    "            batch_tfms += [T.Normalize(*scaled_mnist_stats)] if normalize else []\n",
    "        self.batch_tfms = T.Compose(batch_tfms)\n",
    "\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.current_epoch = epoch\n",
    "        if self.frame_dropout_probs is None or len(self.frame_dropout_probs) == 0:\n",
    "            return\n",
    "\n",
    "        for i in range(len(self.sampler_steps)):\n",
    "            if epoch >= self.sampler_steps[i]:\n",
    "                self.period_idx = i + 1\n",
    "        print(\"set epoch: epoch {} period_idx={}\".format(epoch, self.period_idx))\n",
    "        self.frame_dropout_prob = self.frame_dropout_probs[self.period_idx]\n",
    "\n",
    "    def step_epoch(self):\n",
    "        # one epoch finishes.\n",
    "        print(\"Dataset: epoch {} finishes\".format(self.current_epoch))\n",
    "        self.set_epoch(self.current_epoch + 1)\n",
    "    \n",
    "    def random_place(self, img):\n",
    "        \"Randomly place the digit inside the canvas\"\n",
    "        x = random.uniform(-self.pad, self.pad)\n",
    "        y = random.uniform(-self.pad, self.pad)\n",
    "\n",
    "        center_point = get_affine_transformed_coordinates(\n",
    "            (self.img_size//2, self.img_size//2),\n",
    "            translate=(x,y),\n",
    "            center=(self.img_size//2, self.img_size//2),\n",
    "        )\n",
    "        return TF.affine(img, translate=(x,y), angle=0, scale=1, shear=(0,0)), center_point\n",
    "\n",
    "    def get_digit(self, digit_idx):\n",
    "        \"Get a MNIST digit randomly placed on the canvas\"\n",
    "        img = self.mnist_dataset[[digit_idx]]\n",
    "        pimg = TF.pad(img, padding=self.pad)\n",
    "        img, center_point = self.random_place(pimg)\n",
    "        target = {\n",
    "            'label': int(self.mnist_targets[digit_idx]),\n",
    "            'center_point': center_point\n",
    "        }\n",
    "        return img, target\n",
    "\n",
    "    def _one_moving_digit(self, id):\n",
    "        digit, target = self.get_digit(digit_idx=id)\n",
    "        traj = RandomTrajectory(\n",
    "            self.affine_params, center=(self.img_size//2, self.img_size//2), \n",
    "            n=self.num_frames-1,\n",
    "            bounce=self.bounce,\n",
    "            use_center_bounce=self.use_center_bounce,\n",
    "        )\n",
    "        sequence, points = traj(digit, target['center_point'])\n",
    "\n",
    "        targets = []\n",
    "        for point in points:\n",
    "            targets.append({\n",
    "                'label': target['label'],\n",
    "                'center_point': point\n",
    "            })\n",
    "\n",
    "        return torch.stack(sequence), targets\n",
    "\n",
    "    def generate_sequence(self, idx):\n",
    "      moving_digits_and_targets = [self._one_moving_digit(id) for id in self.ids[idx]]\n",
    "      moving_digits = torch.stack([d[0] for d in moving_digits_and_targets])\n",
    "\n",
    "      combined_digits = moving_digits.max(dim=0)[0]\n",
    "      targets = []\n",
    "\n",
    "      for frame_number in range(self.num_frames):\n",
    "          target = {}\n",
    "\n",
    "          labels = []\n",
    "          center_points = []\n",
    "\n",
    "          for i in range(len(self.ids[idx])):\n",
    "              digit_target = moving_digits_and_targets[i][1][frame_number]\n",
    "\n",
    "              if 'label' in digit_target and digit_target['center_point'][0] >= 0 and digit_target['center_point'][1] >= 0 and digit_target['center_point'][0] < self.img_size and digit_target['center_point'][1] < self.img_size:\n",
    "                labels.append(digit_target['label'])\n",
    "                center_points.append(digit_target['center_point'])\n",
    "\n",
    "          target['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "          target['center_points'] = torch.tensor(center_points, dtype=torch.float32)\n",
    "          targets.append(target)\n",
    "\n",
    "      return combined_digits, targets\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      if self.sequences is None:\n",
    "        images, targets = self.generate_sequence(idx)\n",
    "      else:\n",
    "        images, targets = self.sequences[idx]\n",
    "\n",
    "      targets = copy.deepcopy(targets) # targets dictionary is mutable\n",
    "\n",
    "      if self.keep_frame_mask is not None:\n",
    "        keep_frame_flags = self.keep_frame_mask\n",
    "      else:\n",
    "        num_potential_drop_frames = self.num_frames // 2\n",
    "        frame_keep_probs = torch.rand(num_potential_drop_frames)\n",
    "        keep_frame_flags = (frame_keep_probs > self.frame_dropout_prob).int()\n",
    "        keep_frame_flags = torch.cat([torch.ones(self.num_frames - num_potential_drop_frames), keep_frame_flags])\n",
    "\n",
    "      for frame_number, (img, target) in enumerate(zip(images, targets)):\n",
    "        if target['center_points'].size(0) > 0:\n",
    "          target['center_points'] /= torch.tensor([self.img_size, self.img_size], dtype=torch.float32)\n",
    "\n",
    "          target['labels'] = target['labels'].to(torch.int64)\n",
    "\n",
    "        target['keep_frame'] = keep_frame_flags[frame_number]\n",
    "        target['orig_size'] = torch.as_tensor([int(self.img_size), int(self.img_size)])\n",
    "\n",
    "      images = self.batch_tfms(images)\n",
    "\n",
    "      return images, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.sequences is None:\n",
    "          return len(self.ids)\n",
    "        else:\n",
    "          return len(self.sequences)\n",
    "\n",
    "    def save(self, fname=\"mmnist.pt\"):\n",
    "        data_imgs = []\n",
    "        data_targets = []\n",
    "        for i in progress_bar(range(len(self.ids))):\n",
    "            imgs, targets = self.generate_sequence(i)\n",
    "            data_imgs.append(imgs)\n",
    "\n",
    "            data_targets.append(targets)\n",
    "\n",
    "        print(\"Saving dataset\")\n",
    "        torch.save(\n",
    "            {\n",
    "                'imgs': torch.stack(data_imgs),\n",
    "                'targets': data_targets,\n",
    "            },\n",
    "            f\"{fname}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MOiF4oYBDQZ9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "def build_dataset(split, args, frame_dropout_pattern=None):\n",
    "\n",
    "    dataset_name = args.dataset.lower()\n",
    "    assert dataset_name.startswith('moving-mnist')\n",
    "\n",
    "    if '1digit' in dataset_name:\n",
    "        num_digits = [1]\n",
    "    elif '2digit' in dataset_name:\n",
    "        num_digits = [2]\n",
    "    else:\n",
    "        raise ValueError(f'unknown {dataset_name}')\n",
    "\n",
    "    split_indices_attr = f\"{split}_split_indices\"\n",
    "\n",
    "    if not hasattr(args, split_indices_attr) or getattr(args, f\"{split}_split_indices\") == None or len(getattr(args, f\"{split}_split_indices\")) == 0:\n",
    "        # Load the MNIST dataset only once\n",
    "        full_dataset = MNIST(\".\", download=True)\n",
    "        num_samples = len(full_dataset)\n",
    "        \n",
    "        # Create train and validation indices\n",
    "        indices = list(range(num_samples))\n",
    "        indices_split = int(args.train_val_split_ratio * num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        train_indices = indices[:indices_split]\n",
    "        val_indices = indices[indices_split:]\n",
    "        if args.debug:\n",
    "          downsample_factor = 0.005\n",
    "          print(f'Downsample factor: {downsample_factor}')\n",
    "          val_indices = val_indices[:int(len(val_indices) * downsample_factor)]\n",
    "          train_indices = train_indices[:int(len(train_indices) * downsample_factor)]\n",
    "\n",
    "        args.train_split_indices = train_indices\n",
    "        args.val_split_indices = val_indices\n",
    "\n",
    "    \n",
    "    split_indices = getattr(args, f\"{split}_split_indices\")    \n",
    "\n",
    "    affine_params = SimpleNamespace(\n",
    "      angle=(0, 0),\n",
    "      translate=((-5, 5), (-5, 5)),\n",
    "      scale=(1, 1),\n",
    "      shear=(0, 0),\n",
    "    )\n",
    "    bounce = False\n",
    "    use_center_bounce = False\n",
    "    if split == 'train':\n",
    "        dataset = MovingMNIST(\n",
    "            normalize=True,\n",
    "            bounce=bounce,\n",
    "            use_center_bounce=use_center_bounce,\n",
    "            num_digits=num_digits,\n",
    "            num_frames=args.num_frames,\n",
    "            split_indices=split_indices,\n",
    "            frame_dropout_probs=args.frame_dropout_probs,\n",
    "            sampler_steps=args.sampler_steps,\n",
    "            affine_params=affine_params,\n",
    "        )\n",
    "    elif split == 'val':\n",
    "        dataset = MovingMNIST(\n",
    "            normalize=True,\n",
    "            bounce=bounce,\n",
    "            use_center_bounce=use_center_bounce,\n",
    "            num_digits=num_digits,\n",
    "            num_frames=args.num_frames,\n",
    "            split_indices=split_indices,\n",
    "            frame_dropout_pattern=frame_dropout_pattern,\n",
    "            affine_params=affine_params,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f'unknown {split}')\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "VEe4SneCoD9o"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    batch = list(zip(*batch))\n",
    "    batch[0] = torch.stack(batch[0])\n",
    "    batch[1] = list(batch[1])\n",
    "    return tuple(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYJMR0p0-0QE"
   },
   "source": [
    "### Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2mSX2W4imJq2"
   },
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "\"\"\"\n",
    "Modules to compute the matching cost and solve the corresponding LSAP.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class HungarianMatcher(nn.Module):\n",
    "    \"\"\"This class computes an assignment between the targets and the predictions of the network\n",
    "\n",
    "    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n",
    "    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best\n",
    "    predictions, while the others are un-matched (and thus treated as non-objects).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cost_class: float = 1, cost_center_point: float = 1,\n",
    "                 focal_alpha: float = 0.25, focal_gamma: float = 2.0):\n",
    "        \"\"\"Creates the matcher\n",
    "\n",
    "        Params:\n",
    "            cost_class: This is the relative weight of the classification error in the matching cost\n",
    "            cost_bbox: This is the relative weight of the L1 error of the bounding box coordinates\n",
    "                       in the matching cost\n",
    "            cost_giou: This is the relative weight of the giou loss of the bounding box in the\n",
    "                       matching cost\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cost_class = cost_class\n",
    "\n",
    "        self.cost_center_point = cost_center_point\n",
    "\n",
    "        self.focal_alpha = focal_alpha\n",
    "        self.focal_gamma = focal_gamma\n",
    "\n",
    "        assert cost_class != 0 or cost_center_point != 0, \"all costs cant be 0\"\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, targets):\n",
    "\n",
    "        batch_size, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
    "\n",
    "        # There're 2 cases for this clss\n",
    "        # 1. w/o temporal dimention:\n",
    "        # batch size dimention represents number of independent frames in the batch\n",
    "        # 2. w/ temporal dimention:\n",
    "        # batch size dimention represents number of frames in the sequence\n",
    "        # (we do not support multiple sequences in the batch)\n",
    "\n",
    "        # We flatten to compute the cost matrices in a batch\n",
    "        #\n",
    "        # [batch_size * num_queries, num_classes]\n",
    "        out_prob = outputs[\"pred_logits\"].flatten(0, 1).sigmoid()\n",
    "\n",
    "        # [batch_size * num_queries, 2]\n",
    "        out_center_points = outputs[\"pred_center_points\"].flatten(0, 1)\n",
    "\n",
    "        # Also concat the target labels and boxes\n",
    "        tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n",
    "\n",
    "        tgt_center_points = torch.cat([v[\"center_points\"] for v in targets])\n",
    "\n",
    "        # Compute the classification cost.\n",
    "        neg_cost_class = (1 - self.focal_alpha) * (out_prob ** self.focal_gamma) * (-(1 - out_prob + 1e-8).log())\n",
    "        pos_cost_class = self.focal_alpha * ((1 - out_prob) ** self.focal_gamma) * (-(out_prob + 1e-8).log())\n",
    "\n",
    "        # tgt_ids - concatenated GT label ids\n",
    "        # Per each query we contains logits per all labes\n",
    "        # [batch_size * num_queries, batch_size]\n",
    "        cost_class = pos_cost_class[:, tgt_ids] - neg_cost_class[:, tgt_ids]\n",
    "\n",
    "        # Compute the L1 cost between center points\n",
    "        cost_center_points = torch.cdist(out_center_points, tgt_center_points, p=1)\n",
    "\n",
    "        # Final cost matrix\n",
    "        # [batch_size * num_queries, batch_size]\n",
    "        cost_matrix = self.cost_class * cost_class \\\n",
    "              + self.cost_center_point * cost_center_points\n",
    "\n",
    "        # [batch_size, num_queries, batch_size]\n",
    "        cost_matrix = cost_matrix.view(batch_size, num_queries, -1).cpu()\n",
    "\n",
    "        # Number of GTs per each batch index\n",
    "        # [batch_size]\n",
    "        sizes = [len(v[\"labels\"]) for v in targets] # Changed from targets to labels\n",
    "\n",
    "        # Split returns a tuple where each element has hape [batch_size, num_queries, size]\n",
    "        # During enumeration we assign particular batch index between query and GT size\n",
    "        indices = [linear_sum_assignment(c[i])\n",
    "                   for i, c in enumerate(cost_matrix.split(sizes, -1))]\n",
    "\n",
    "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64))\n",
    "                for i, j in indices]\n",
    "\n",
    "\n",
    "def build_matcher():\n",
    "    return HungarianMatcher(\n",
    "        cost_class = 2, \n",
    "        cost_center_point = 5,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "HgE6xsXHFeL-"
   },
   "outputs": [],
   "source": [
    "# b = torch.tensor([1, 1, 1, 1, 1, 1, 1, 3]) # sizes\n",
    "# a = torch.arange(0, 8*4*b.sum().item()).reshape(8, 4, -1)\n",
    "# _split = a.split(b.tolist(),-1)\n",
    "# indices = [linear_sum_assignment(c[i])\n",
    "#             for i, c in enumerate(a.split(b.tolist(),-1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-nbeUN7g-1qi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "class SetCriterion(nn.Module):\n",
    "    def __init__(self, num_classes, matcher, focal_alpha, focal_gamma, weight_dict):\n",
    "        super(SetCriterion, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.coord_criterion = nn.MSELoss(reduction='sum')\n",
    "        self.class_criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "        self.matcher = matcher\n",
    "        self.focal_alpha = focal_alpha\n",
    "        self.focal_gamma = focal_gamma\n",
    "        self.weight_dict = weight_dict\n",
    "\n",
    "    def loss_labels_focal(self, outputs, targets, indices, num_objects, log=True):\n",
    "        \"\"\"Classification loss (NLL)\n",
    "        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_boxes]\n",
    "        \"\"\"\n",
    "        assert 'pred_logits' in outputs\n",
    "\n",
    "        # [batch_size, number_queries, number_of_classes]\n",
    "        src_logits = outputs['pred_logits']\n",
    "\n",
    "        # (batch_ids, output_query_ids)\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "\n",
    "        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\n",
    "\n",
    "        # [batch_size, number_queries]\n",
    "        target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
    "                                    dtype=torch.int64, device=src_logits.device)\n",
    "\n",
    "        target_classes[idx] = target_classes_o\n",
    "\n",
    "        # [batch_size, number_queries, number_of_classes+1]\n",
    "        target_classes_onehot = torch.zeros([src_logits.shape[0], src_logits.shape[1], src_logits.shape[2] + 1],\n",
    "                                            dtype=src_logits.dtype, layout=src_logits.layout, device=src_logits.device)\n",
    "\n",
    "        #\n",
    "        target_classes_onehot.scatter_(2, target_classes.unsqueeze(-1), 1)\n",
    "\n",
    "        # [batch_size, number_queries, number_of_classes]\n",
    "        target_classes_onehot = target_classes_onehot[:,:,:-1]\n",
    "\n",
    "        loss_ce = sigmoid_focal_loss(\n",
    "            src_logits, target_classes_onehot, num_objects,\n",
    "            alpha=self.focal_alpha, gamma=self.focal_gamma)\n",
    "\n",
    "        loss_ce *= src_logits.shape[1] # Why?\n",
    "        losses = {'loss_ce': loss_ce}\n",
    "\n",
    "        if log:\n",
    "            # TODO this should probably be a separate loss, not hacked in this one here\n",
    "            losses['class_error'] = 100 - accuracy(src_logits[idx], target_classes_o)[0]\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def loss_center_points(self, outputs, targets, indices, num_objects, log=True):\n",
    "        \"\"\"L1 center point loss\n",
    "        targets dicts must contain the key \"center_points\" containing a tensor of dim [nb_target_boxes, 2]\n",
    "        \"\"\"\n",
    "        assert 'pred_center_points' in outputs\n",
    "\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        src_cps = outputs['pred_center_points'][idx]\n",
    "        target_cps = torch.cat([t['center_points'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
    "\n",
    "        loss_cp = F.l1_loss(src_cps, target_cps, reduction='none')\n",
    "\n",
    "        losses = {}\n",
    "        losses['loss_center_point'] = loss_cp.sum() / num_objects\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        indecies = self.matcher(outputs, targets)\n",
    "\n",
    "        num_objects = sum(len(t[\"labels\"]) for t in targets)\n",
    "        num_objects = torch.as_tensor(\n",
    "            [num_objects], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
    "\n",
    "        losses = {}\n",
    "        loss_labels = self.loss_labels_focal(outputs, targets, indecies, num_objects)\n",
    "        loss_center_points = self.loss_center_points(outputs, targets, indecies, num_objects)\n",
    "\n",
    "        losses.update(loss_labels)\n",
    "        losses.update(loss_center_points)\n",
    "        return losses\n",
    "\n",
    "    def _get_src_permutation_idx(self, indices):\n",
    "        # permute predictions following indices\n",
    "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
    "        src_idx = torch.cat([src for (src, _) in indices])\n",
    "        return batch_idx, src_idx\n",
    "\n",
    "    def _get_tgt_permutation_idx(self, indices):\n",
    "        # permute targets following indices\n",
    "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
    "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
    "        return batch_idx, tgt_idx\n",
    "\n",
    "def build_criterion(num_classes):\n",
    "    matcher = build_matcher()\n",
    "    return SetCriterion(\n",
    "        num_classes,\n",
    "        matcher,\n",
    "        focal_alpha=0.25,\n",
    "        focal_gamma=2,\n",
    "        weight_dict={\n",
    "            'loss_ce': 2,\n",
    "            'loss_center_point': 5\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "GowZNn49xxZd"
   },
   "outputs": [],
   "source": [
    "def sigmoid_focal_loss(inputs, targets, num_boxes, alpha: float = 0.25, gamma: float = 2, query_mask=None, reduction=True):\n",
    "    \"\"\"\n",
    "    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n",
    "    Args:\n",
    "        inputs: A float tensor of arbitrary shape.\n",
    "                The predictions for each example.\n",
    "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
    "                 classification label for each element in inputs\n",
    "                (0 for the negative class and 1 for the positive class).\n",
    "        alpha: (optional) Weighting factor in range (0,1) to balance\n",
    "                positive vs negative examples. Default = -1 (no weighting).\n",
    "        gamma: Exponent of the modulating factor (1 - p_t) to\n",
    "               balance easy vs hard examples.\n",
    "    Returns:\n",
    "        Loss tensor\n",
    "    \"\"\"\n",
    "    prob = inputs.sigmoid()\n",
    "    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
    "    p_t = prob * targets + (1 - prob) * (1 - targets)\n",
    "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
    "\n",
    "    if alpha >= 0:\n",
    "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
    "        loss = alpha_t * loss\n",
    "\n",
    "    if not reduction:\n",
    "        return loss\n",
    "\n",
    "    if query_mask is not None:\n",
    "        loss = torch.stack([l[m].mean(0) for l, m in zip(loss, query_mask)])\n",
    "        return loss.sum() / num_boxes\n",
    "    return loss.mean(1).sum() / num_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "EpvdfJ_0xz8p"
   },
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    if target.numel() == 0:\n",
    "        return [torch.zeros([], device=output.device)]\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oo78zHtTycNk"
   },
   "source": [
    "### Postprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "yoHXrXNkye7x"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class AverageDisplacementErrorEvaluator:\n",
    "\n",
    "    def __init__(self, matcher, output_dir, prefix='', img_size = 1):\n",
    "        self.matcher = matcher  # Matcher for finding the best target for each prediction\n",
    "        self.ADE = []  # List to store displacement errors\n",
    "        self.prefix = prefix  # Prefix for summary keys\n",
    "        self.result = None  # Placeholder for accumulated result\n",
    "        self.output_dir = output_dir\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def update(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        Updates the evaluator with outputs and targets by computing the displacement errors.\n",
    "\n",
    "        Parameters:\n",
    "        outputs (dict): Dictionary containing the model's predicted outputs.\n",
    "        targets (list): List of dictionaries containing the ground-truth target data.\n",
    "        \"\"\"\n",
    "        # Match predictions to targets\n",
    "        indices = self.matcher(outputs, targets)\n",
    "\n",
    "        # Permute predictions based on matched indices\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        src_cps = outputs['pred_center_points'][idx] # Predicted center points\n",
    "        src_cps *= torch.tensor([self.img_size, self.img_size], dtype=torch.float32)\n",
    "        \n",
    "        target_cps = torch.cat([t['center_points'][i] for t, (_, i) in zip(targets, indices)], dim=0)  # True center points\n",
    "        target_cps *= torch.tensor([self.img_size, self.img_size], dtype=torch.float32)\n",
    "\n",
    "        displacements = torch.norm(src_cps - target_cps, dim=1).detach().cpu()  # Shape: [N, 1]\n",
    "\n",
    "        batch_idx, _ = idx\n",
    "\n",
    "        for i in range(len(batch_idx)):\n",
    "            idx = batch_idx[i].item()\n",
    "            \n",
    "            if len(self.ADE) == idx:\n",
    "                # no list for the timestamp (or batch id)\n",
    "                self.ADE.append([])\n",
    "            \n",
    "            assert self.ADE[idx] is not None, \"List for the batch index should exist\"\n",
    "            self.ADE[idx].append(displacements[i].item())\n",
    "\n",
    "    def accumulate(self):\n",
    "        \"\"\"\n",
    "        Computes the final accumulated result by averaging all stored errors.\n",
    "        \"\"\"\n",
    "        self.result = []\n",
    "        for displacements_per_timestamp in self.ADE:\n",
    "            mean = float(np.mean(displacements_per_timestamp))\n",
    "            self.result.append(mean)\n",
    "\n",
    "    def summary(self):\n",
    "        \"\"\"\n",
    "        Returns a summary of the results.\n",
    "\n",
    "        Returns:\n",
    "        dict: Dictionary containing the ADE result with the prefix as a key.\n",
    "        \"\"\"\n",
    "        timestamps = list(range(len(self.result))) \n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(timestamps, self.result, marker='o', label='ADE')\n",
    "        plt.title(f'ADE vs. Timestamp{\" (\" + self.prefix + \")\" if self.prefix else \"\"}')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.ylabel('ADE')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the plot\n",
    "        plt.savefig(self.output_dir)\n",
    "        plt.close()\n",
    "\n",
    "        return {f'{self.prefix+\"_\" if self.prefix else \"\"}ADE_{t}': ade for t, ade in enumerate(self.result)}\n",
    "\n",
    "    def _get_src_permutation_idx(self, indices):\n",
    "      # permute predictions following indices\n",
    "      batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
    "      src_idx = torch.cat([src for (src, _) in indices])\n",
    "      return batch_idx, src_idx\n",
    "\n",
    "\n",
    "class PostProcessTrajectory(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, targets):\n",
    "        outputs = {\n",
    "            k: v.detach().cpu() for k, v in outputs.items()\n",
    "        }\n",
    "\n",
    "        targets = [{k: v.detach().cpu() for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        return outputs, targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnH6MFGhtFh1"
   },
   "source": [
    "### Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXJe2q-wb6mr"
   },
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "nBaeIAdOsf3-"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torchmetrics\n",
    "from collections import defaultdict\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, epoch, device):\n",
    "    model.train()\n",
    "    metric_logger = defaultdict(lambda: torchmetrics.MeanMetric().to(device))\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch}\", leave=True)\n",
    "\n",
    "    print_freq = 50\n",
    "\n",
    "    for i, (samples, targets) in enumerate(progress_bar):\n",
    "        samples = samples.to(device)\n",
    "\n",
    "        targets = [[{k: v.to(device) for k, v in t.items()} for t in batch_targets] for batch_targets in targets]\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        out, targets_flat = model(samples, targets)\n",
    "        loss_dict = criterion(out, targets_flat)\n",
    "\n",
    "        weight_dict = criterion.weight_dict\n",
    "        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "\n",
    "        # Logic related to reduce\n",
    "        loss_dict_unscaled = {\n",
    "            f'{k}_unscaled': v for k, v in loss_dict.items()}\n",
    "        loss_dict_scaled = {\n",
    "            k: v * weight_dict[k] for k, v in loss_dict.items() if k in weight_dict}\n",
    "        losses_scaled = sum(loss_dict_scaled.values())\n",
    "\n",
    "        loss_value = losses_scaled.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training\")\n",
    "            print(loss_dict)\n",
    "            sys.exit(1)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        losses.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Add gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update metric logger with main loss and each component\n",
    "        metric_logger['loss'].update(loss_value)\n",
    "        metric_logger[\"class_error\"].update(loss_dict['class_error'].item())\n",
    "\n",
    "        for k, v in loss_dict_unscaled.items():\n",
    "            metric_logger[k].update(v.item())\n",
    "        for k, v in loss_dict_scaled.items():\n",
    "            metric_logger[k].update(v.item())\n",
    "\n",
    "        if i % print_freq == 0 or i == len(dataloader) - 1:\n",
    "            progress_bar.set_postfix({\n",
    "                **{k: metric.compute().item() for k, metric in metric_logger.items()},\n",
    "                \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                'frame_dropout_prob': dataloader.dataset.frame_dropout_prob,\n",
    "            })\n",
    "\n",
    "    avg_values = {k: metric.compute().item() for k, metric in metric_logger.items()}\n",
    "\n",
    "    for metric in metric_logger.values():\n",
    "      metric.reset()\n",
    "\n",
    "    return avg_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_X021ox7b0Cg"
   },
   "source": [
    "#### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "JM9WyssNlv_Q"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(model, dataloader, criterion, postprocessors, epoch, device, output_dir, experiment='default'):\n",
    "\n",
    "    average_displacement_error_evaluator = None\n",
    "    if 'trajectory' in postprocessors:\n",
    "      average_displacement_error_evaluator = AverageDisplacementErrorEvaluator(\n",
    "          output_dir=f'{output_dir}/ade_{experiment}_epoch-{epoch}.png',\n",
    "          prefix=experiment,\n",
    "          matcher=criterion.matcher,\n",
    "          img_size=dataloader.dataset.img_size,\n",
    "        )\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    print_freq = 50\n",
    "\n",
    "    metric_logger = defaultdict(lambda: torchmetrics.MeanMetric().to(device))\n",
    "\n",
    "    # Disabling gradient calculation for evaluation\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Eval {epoch}:\", leave=True)\n",
    "\n",
    "        for i, (samples, targets) in enumerate(progress_bar):\n",
    "            samples = samples.to(device)\n",
    "            targets = [[{k: v.to(device) for k, v in t.items()} for t in batch_targets] for batch_targets in targets]\n",
    "\n",
    "            # Forward pass\n",
    "            out, targets_flat = model(samples, targets)\n",
    "\n",
    "            loss_dict = criterion(out, targets_flat)\n",
    "\n",
    "            weight_dict = criterion.weight_dict\n",
    "            losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "\n",
    "            # Logic related to reduce\n",
    "            loss_dict_unscaled = {\n",
    "                f'{k}_unscaled': v for k, v in loss_dict.items()}\n",
    "            loss_dict_scaled = {\n",
    "                k: v * weight_dict[k] for k, v in loss_dict.items() if k in weight_dict}\n",
    "            losses_scaled = sum(loss_dict_scaled.values())\n",
    "\n",
    "            loss_value = losses_scaled.item()\n",
    "\n",
    "            metric_logger['loss'].update(loss_value)\n",
    "            metric_logger['class_error'].update(loss_dict['class_error'].item())\n",
    "\n",
    "            for k, v in loss_dict_unscaled.items():\n",
    "              metric_logger[k].update(v.item())\n",
    "            for k, v in loss_dict_scaled.items():\n",
    "              metric_logger[k].update(v.item())\n",
    "\n",
    "            if i % print_freq == 0 or i == len(dataloader) - 1:\n",
    "              progress_bar.set_postfix({\n",
    "                  **{k: metric.compute().item() for k, metric in metric_logger.items()},\n",
    "                  'frame_dropout_prob': dataloader.dataset.frame_dropout_prob,\n",
    "              })\n",
    "\n",
    "            if average_displacement_error_evaluator:\n",
    "              average_displacement_error_evaluator.update(*postprocessors['trajectory'](out, targets_flat))\n",
    "\n",
    "    avg_values = {k: metric.compute().item() for k, metric in metric_logger.items()}\n",
    "\n",
    "    if average_displacement_error_evaluator:\n",
    "      average_displacement_error_evaluator.accumulate()\n",
    "      avg_values.update(average_displacement_error_evaluator.summary())\n",
    "\n",
    "    for metric in metric_logger.values():\n",
    "      metric.reset()\n",
    "\n",
    "    return avg_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXYwlQbhdh6l"
   },
   "source": [
    "### Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "6ozLlwYNdkE2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_suffix = '_train'\n",
    "val_suffix = '_val'\n",
    "extension = '.pt'\n",
    "\n",
    "dataset_dir = f'/gpfs/helios/home/ploter/projects/datasets/moving-mnist/'\n",
    "\n",
    "if not os.path.exists(dataset_dir):\n",
    "  os.makedirs(dataset_dir)\n",
    "\n",
    "dataset_name_1d_tr = 'moving-mnist-1digit-tr'\n",
    "dataset_name_2d_tr = 'moving-mnist-2digit-tr'\n",
    "\n",
    "if not os.path.exists(os.path.join(dataset_dir, dataset_name_1d_tr+train_suffix+extension)):\n",
    "  print(f'Generating datasets {dataset_name_1d_tr}')\n",
    "\n",
    "  train_val_split_ratio = 0.8\n",
    "  # Load the MNIST dataset to get the total number of samples\n",
    "  full_dataset = MNIST(\".\", download=True)\n",
    "  num_samples = len(full_dataset)\n",
    "\n",
    "  # Generate train and validation indices\n",
    "  indices = list(range(num_samples))\n",
    "  split_index = int(train_val_split_ratio * num_samples)\n",
    "  np.random.shuffle(indices)\n",
    "\n",
    "  train_indices = indices[:split_index]\n",
    "  val_indices = indices[split_index:]\n",
    "\n",
    "  affine_params = SimpleNamespace(\n",
    "      angle=(0, 0),\n",
    "      translate=((-5, 5), (-5, 5)),\n",
    "      scale=(1, 1),\n",
    "      shear=(0, 0),\n",
    "  )\n",
    "  number_frames = 8\n",
    "  num_digits = [1]\n",
    "  img_size = 64\n",
    "\n",
    "  dataset_train = MovingMNIST(\n",
    "      affine_params=affine_params, num_frames=number_frames, num_digits=num_digits, img_size=img_size,\n",
    "      normalize=False,\n",
    "      split_indices=train_indices,\n",
    "      )\n",
    "\n",
    "  dataset_train.save(fname=os.path.join(dataset_dir, dataset_name_1d_tr+train_suffix+extension))\n",
    "\n",
    "  dataset_val = MovingMNIST(\n",
    "      affine_params=affine_params, num_frames=number_frames, num_digits=num_digits, img_size=img_size,\n",
    "      normalize=False,\n",
    "      split_indices=val_indices,\n",
    "      )\n",
    "\n",
    "  dataset_val.save(fname=os.path.join(dataset_dir, dataset_name_1d_tr+val_suffix+extension))\n",
    "\n",
    "  downsample_factor = 0.005\n",
    "  print(f'Downsample factor: {downsample_factor}')\n",
    "  val_indices = val_indices[:int(len(val_indices) * downsample_factor)]\n",
    "  train_indices = train_indices[:int(len(train_indices) * downsample_factor)]\n",
    "\n",
    "  dataset_train = MovingMNIST(\n",
    "      affine_params=affine_params, num_frames=number_frames, num_digits=num_digits, img_size=img_size,\n",
    "      normalize=False,\n",
    "      split_indices=train_indices,\n",
    "      )\n",
    "\n",
    "  dataset_train.save(fname=os.path.join(dataset_dir, dataset_name_1d_tr+'_debug'+train_suffix+extension))\n",
    "\n",
    "  dataset_val = MovingMNIST(\n",
    "      affine_params=affine_params, num_frames=number_frames, num_digits=num_digits, img_size=img_size,\n",
    "      normalize=False,\n",
    "      split_indices=val_indices,\n",
    "      )\n",
    "\n",
    "  dataset_val.save(fname=os.path.join(dataset_dir, dataset_name_1d_tr+'_debug'+val_suffix+extension))\n",
    "\n",
    "\n",
    "if not os.path.exists(os.path.join(dataset_dir, dataset_name_2d_tr+train_suffix+extension)):\n",
    "  print(f'Generating datasets {dataset_name_2d_tr}')\n",
    "\n",
    "  train_val_split_ratio = 0.8\n",
    "  # Load the MNIST dataset to get the total number of samples\n",
    "  full_dataset = MNIST(\".\", download=True)\n",
    "  num_samples = len(full_dataset)\n",
    "\n",
    "  # Generate train and validation indices\n",
    "  indices = list(range(num_samples))\n",
    "  split_index = int(train_val_split_ratio * num_samples)\n",
    "  np.random.shuffle(indices)\n",
    "\n",
    "  train_indices = indices[:split_index]\n",
    "  val_indices = indices[split_index:]\n",
    "\n",
    "  affine_params = SimpleNamespace(\n",
    "      angle=(0, 0),\n",
    "      translate=((-5, 5), (-5, 5)),\n",
    "      scale=(1, 1),\n",
    "      shear=(0, 0),\n",
    "  )\n",
    "  number_frames = 8\n",
    "  num_digits = [2]\n",
    "  img_size = 64\n",
    "\n",
    "  dataset_train = MovingMNIST(\n",
    "      affine_params=affine_params, num_frames=number_frames, num_digits=num_digits, img_size=img_size,\n",
    "      normalize=False,\n",
    "      split_indices=train_indices,\n",
    "      )\n",
    "\n",
    "  dataset_train.save(fname=os.path.join(dataset_dir, dataset_name_2d_tr+train_suffix+extension))\n",
    "\n",
    "  dataset_val = MovingMNIST(\n",
    "      affine_params=affine_params, num_frames=number_frames, num_digits=num_digits, img_size=img_size,\n",
    "      normalize=False,\n",
    "      split_indices=val_indices,\n",
    "      )\n",
    "\n",
    "  dataset_val.save(fname=os.path.join(dataset_dir, dataset_name_2d_tr+val_suffix+extension))\n",
    "\n",
    "  downsample_factor = 0.005\n",
    "  print(f'Downsample factor: {downsample_factor}')\n",
    "  val_indices = val_indices[:int(len(val_indices) * downsample_factor)]\n",
    "  train_indices = train_indices[:int(len(train_indices) * downsample_factor)]\n",
    "\n",
    "  dataset_train = MovingMNIST(\n",
    "      affine_params=affine_params, num_frames=number_frames, num_digits=num_digits, img_size=img_size,\n",
    "      normalize=False,\n",
    "      split_indices=train_indices,\n",
    "      )\n",
    "\n",
    "  dataset_train.save(fname=os.path.join(dataset_dir, dataset_name_2d_tr+'_debug'+train_suffix+extension))\n",
    "\n",
    "  dataset_val = MovingMNIST(\n",
    "      affine_params=affine_params, num_frames=number_frames, num_digits=num_digits, img_size=img_size,\n",
    "      normalize=False,\n",
    "      split_indices=val_indices,\n",
    "      )\n",
    "\n",
    "  dataset_val.save(fname=os.path.join(dataset_dir, dataset_name_2d_tr+'_debug'+val_suffix+extension))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_Hf9FcTzced"
   },
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.seed = 42\n",
    "        self.batch_size = 1\n",
    "        self.epochs = 14\n",
    "        self.learning_rate = 1e-3\n",
    "        self.weight_decay = 0.01\n",
    "        self.num_objects = 4 # TODO: refactor it\n",
    "        self.num_classes = 10\n",
    "        self.model = 'lstm'\n",
    "        self.lstm_hidden_size = 128\n",
    "        self.debug = False\n",
    "        self.experiment = 'experiment_2024-12-06_v4'\n",
    "\n",
    "        self.resume = None  # or 'checkpoint_epoch_10.pth'\n",
    "        self.num_frames = 8,\n",
    "        self.frame_dropout_probs = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "        self.sampler_steps = [2, 4, 6, 8, 10]\n",
    "        self.frame_dropout_pattern = '00001111'\n",
    "        self.output_dir = '/gpfs/helios/home/ploter/projects/MovingMNIST/models'\n",
    "        self.dataset = 'moving-mnist-2digit-tr'\n",
    "\n",
    "        self.train_val_split_ratio = 0.8\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Perceiver model specific arguments\n",
    "        self.num_freq_bands = 6          # Number of frequency bands for Fourier encoding\n",
    "        self.max_freq = 10              # Maximum frequency for Fourier encoding\n",
    "        self.enc_layers = 1             # Depth of the Perceiver encoder\n",
    "        \n",
    "        self.hidden_dim = 128           # Latent dimension\n",
    "        self.enc_nheads_cross = 1       # Number of cross-attention heads\n",
    "        self.nheads = 1                 # Number of latent self-attention heads\n",
    "        self.dropout = 0.0              # Dropout rate\n",
    "        self.self_per_cross_attn = 1    # Number of self-attention blocks per cross-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "\n",
    "def main(args):\n",
    "    # Set seeds for reproducibility\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    random.seed(args.seed)\n",
    "\n",
    "    # Paths and directories\n",
    "    output_dir = f'{args.output_dir}/{args.model}/{args.experiment}'\n",
    "\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Dataset and dataloaders\n",
    "    dataset_train = build_dataset('train', args)\n",
    "    dataset_val = build_dataset('val', args)\n",
    "    dataset_val_blind = build_dataset('val', args, frame_dropout_pattern=args.frame_dropout_pattern)\n",
    "\n",
    "    sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "    sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "    sampler_val_blind = torch.utils.data.SequentialSampler(dataset_val_blind)\n",
    "\n",
    "    dataloader_train = DataLoader(dataset_train, sampler=sampler_train, batch_size=args.batch_size, collate_fn=collate_fn)\n",
    "    dataloader_val = DataLoader(dataset_val, sampler=sampler_val, batch_size=args.batch_size, collate_fn=collate_fn)\n",
    "    dataloader_val_blind = DataLoader(dataset_val_blind, sampler=sampler_val_blind, batch_size=args.batch_size, collate_fn=collate_fn)\n",
    "\n",
    "    # Model, criterion, optimizer, and scheduler\n",
    "    model = build_model(args)\n",
    "    postprocessors = {'trajectory': PostProcessTrajectory()}\n",
    "\n",
    "    param_dicts = [\n",
    "        {\"params\": [p for n, p in model.named_parameters() if p.requires_grad], \"lr\": args.learning_rate},\n",
    "    ]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(param_dicts, lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "    criterion = build_criterion(args.num_classes)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=12, gamma=0.1)\n",
    "\n",
    "    # Resume from checkpoint\n",
    "    if args.resume:\n",
    "        checkpoint_path = Path(output_dir) / args.resume\n",
    "        print(f'Resuming from {checkpoint_path}')\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_loss = checkpoint['best_val_loss']\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        best_val_loss = float('inf')\n",
    "\n",
    "    model = model.to(args.device)\n",
    "\n",
    "    for state in optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.to(args.device)\n",
    "    \n",
    "    criterion = criterion.to(args.device)\n",
    "\n",
    "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('Number of parameters:', n_parameters)\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Start training\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    dataset_train.set_epoch(start_epoch)\n",
    "    dataset_val.set_epoch(start_epoch)\n",
    "    dataset_val_blind.set_epoch(start_epoch)\n",
    "\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        train_stats = train_one_epoch(model, dataloader_train, optimizer, criterion, epoch, args.device)\n",
    "        \n",
    "        test_stats = evaluate(model, dataloader_val, criterion, postprocessors, \n",
    "                              epoch, args.device, output_dir, experiment='default')\n",
    "        blind_stats = evaluate(model, dataloader_val_blind, criterion, postprocessors, \n",
    "                               epoch, args.device, output_dir, experiment='blind')\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        checkpoint_path = Path(output_dir) / f\"checkpoint_epoch_{epoch}.pth\"\n",
    "        val_loss = test_stats.get(\"loss\", float(\"inf\"))\n",
    "\n",
    "        log_stats = {\n",
    "            **{f'train_{k}': v for k, v in train_stats.items()},\n",
    "            **{f'test_{k}': v for k, v in test_stats.items()},\n",
    "            **{f'test_{k}': v for k, v in blind_stats.items() if 'ADE' in k},\n",
    "            'epoch': epoch,\n",
    "            'n_parameters': n_parameters,\n",
    "            'frame_dropout_prob': dataset_train.frame_dropout_prob\n",
    "        }\n",
    "\n",
    "        if output_dir:\n",
    "            with (Path(output_dir) / \"log.txt\").open(\"a\") as f:\n",
    "                f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "        if val_loss < best_val_loss or epoch % 2 == 0 or epoch + 1 == args.epochs:\n",
    "            torch.save({\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'best_val_loss': best_val_loss\n",
    "            }, checkpoint_path)\n",
    "            best_val_loss = val_loss\n",
    "            print(f\"Checkpoint saved at epoch {epoch} with val loss {val_loss:.4f}\")\n",
    "\n",
    "        dataset_train.step_epoch()\n",
    "        dataset_val.step_epoch()\n",
    "        dataset_val_blind.step_epoch()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print('Training time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7mnqwU28GSgk",
    "outputId": "0dc5b9c6-a57e-4a37-a763-da95fda0ad5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device: {device}')\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment: train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampler_steps=[] frame_dropout_probs=[]\n",
      "New computed stats for MovingMNIST: ([0.050148437500000004], [0.11790625])\n",
      "sampler_steps=[] frame_dropout_probs=[]\n",
      "New computed stats for MovingMNIST: ([0.050148437500000004], [0.11790625])\n",
      "sampler_steps=[] frame_dropout_probs=[]\n",
      "Disable probability based frame drops. Use frame drops based on fixed mask.\n",
      "Set frame keep mask: tensor([1, 1, 1, 1, 0, 0, 0, 0])\n",
      "New computed stats for MovingMNIST: ([0.050148437500000004], [0.11790625])\n",
      "Number of parameters: 702768\n",
      "Start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 3/48000 [00:01<7:24:19,  1.80it/s, loss=8.42, class_error=100, loss_ce_unscaled=2.48, class_error_unscaled=100, loss_center_point_unscaled=0.693, loss_ce=4.95, loss_center_point=3.47, lr=0.001, frame_dropout_prob=0] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     args\u001b[38;5;241m.\u001b[39mexperiment \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_debug\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     22\u001b[0m     args\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_debug\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 80\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     77\u001b[0m dataset_val_blind\u001b[38;5;241m.\u001b[39mset_epoch(start_epoch)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, args\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m---> 80\u001b[0m     train_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     test_stats \u001b[38;5;241m=\u001b[39m evaluate(model, dataloader_val, criterion, postprocessors, \n\u001b[1;32m     83\u001b[0m                           epoch, args\u001b[38;5;241m.\u001b[39mdevice, output_dir, experiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     84\u001b[0m     blind_stats \u001b[38;5;241m=\u001b[39m evaluate(model, dataloader_val_blind, criterion, postprocessors, \n\u001b[1;32m     85\u001b[0m                            epoch, args\u001b[38;5;241m.\u001b[39mdevice, output_dir, experiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblind\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 45\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, criterion, epoch, device)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Backpropagation and optimization\u001b[39;00m\n\u001b[1;32m     43\u001b[0m losses\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 45\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Add gradient clipping\u001b[39;00m\n\u001b[1;32m     46\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Update metric logger with main loss and each component\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/MovingMNIST/mmnist/lib/python3.9/site-packages/torch/nn/utils/clip_grad.py:30\u001b[0m, in \u001b[0;36m_no_grad.<locals>._no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_no_grad_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/MovingMNIST/mmnist/lib/python3.9/site-packages/torch/nn/utils/clip_grad.py:114\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (device, _), ([device_grads], _) \u001b[38;5;129;01min\u001b[39;00m grouped_grads\u001b[38;5;241m.\u001b[39mitems():  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m _has_foreach_support(device_grads, device)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    112\u001b[0m         foreach \u001b[38;5;129;01mand\u001b[39;00m _device_has_foreach_support(device)\n\u001b[1;32m    113\u001b[0m     ):\n\u001b[0;32m--> 114\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_mul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_coef_clamped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m foreach:\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    117\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach=True was passed, but can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt use the foreach API on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = Args()\n",
    "\n",
    "args.model = 'lstm'\n",
    "args.epochs = 15\n",
    "args.learning_rate = 1e-3\n",
    "args.experiment = 'experiment_2024-12-18_v1'\n",
    "args.dataset = 'moving-mnist-2digit-tr'\n",
    "args.num_frames = 8\n",
    "\n",
    "#args.frame_dropout_probs = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "#args.sampler_steps =             [2, 4, 6, 8, 10]\n",
    "args.frame_dropout_probs = []\n",
    "args.sampler_steps = []\n",
    "\n",
    "#args.resume = 'checkpoint_epoch_10.pth'\n",
    "\n",
    "args.debug = False\n",
    "args.device = device\n",
    "\n",
    "if args.debug:\n",
    "    args.experiment += '_debug'\n",
    "    args.dataset += '_debug'\n",
    "\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment: train perceiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampler_steps=[2, 4, 6, 8, 10] frame_dropout_probs=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
      "New computed stats for MovingMNIST: ([0.050148437500000004], [0.11790625])\n",
      "sampler_steps=[] frame_dropout_probs=[]\n",
      "New computed stats for MovingMNIST: ([0.050148437500000004], [0.11790625])\n",
      "sampler_steps=[] frame_dropout_probs=[]\n",
      "Disable probability based frame drops. Use frame drops based on fixed mask.\n",
      "Set frame keep mask: tensor([1, 1, 1, 1, 0, 0, 0, 0])\n",
      "New computed stats for MovingMNIST: ([0.050148437500000004], [0.11790625])\n",
      "num_freq_bands: 6\n",
      "depth: 1\n",
      "max_freq: 10\n",
      "input_channels: 1\n",
      "input_axis: 2\n",
      "num_latents: 4\n",
      "num_classes: -1\n",
      "weight_tie_layers: False\n",
      "fourier_encode_data: True\n",
      "self_per_cross_attn: 1\n",
      "final_classifier_head: False\n",
      "attn_dropout: 0.0\n",
      "cross_dim_head: 27\n",
      "cross_heads: 1\n",
      "ff_dropout: 0.0\n",
      "latent_dim: 128\n",
      "latent_dim_head: 128\n",
      "latent_heads: 1\n",
      "__class__: <class '__main__.Perceiver'>\n",
      "Resuming from /gpfs/helios/home/ploter/projects/MovingMNIST/models/perceiver/experiment_2024-12-16_v2/checkpoint_epoch_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1264943/3322909488.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 505973\n",
      "Start training\n",
      "set epoch: epoch 11 period_idx=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11:   0%|          | 60/48000 [00:02<37:59, 21.03it/s, loss=0.676, class_error=53.2, loss_ce_unscaled=0.244, class_error_unscaled=53.2, loss_center_point_unscaled=0.0377, loss_ce=0.488, loss_center_point=0.188, lr=0.001, frame_dropout_prob=0.6]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     args\u001b[38;5;241m.\u001b[39mexperiment \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_debug\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     20\u001b[0m     args\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_debug\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 80\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     77\u001b[0m dataset_val_blind\u001b[38;5;241m.\u001b[39mset_epoch(start_epoch)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, args\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m---> 80\u001b[0m     train_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     test_stats \u001b[38;5;241m=\u001b[39m evaluate(model, dataloader_val, criterion, postprocessors, \n\u001b[1;32m     83\u001b[0m                           epoch, args\u001b[38;5;241m.\u001b[39mdevice, output_dir, experiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     84\u001b[0m     blind_stats \u001b[38;5;241m=\u001b[39m evaluate(model, dataloader_val_blind, criterion, postprocessors, \n\u001b[1;32m     85\u001b[0m                            epoch, args\u001b[38;5;241m.\u001b[39mdevice, output_dir, experiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblind\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 43\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, criterion, epoch, device)\u001b[0m\n\u001b[1;32m     40\u001b[0m     sys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Backpropagation and optimization\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)  \u001b[38;5;66;03m# Add gradient clipping\u001b[39;00m\n\u001b[1;32m     46\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/projects/MovingMNIST/mmnist/lib/python3.9/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/MovingMNIST/mmnist/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/MovingMNIST/mmnist/lib/python3.9/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = Args()\n",
    "\n",
    "args.model = 'perceiver'\n",
    "args.epochs = 15\n",
    "args.learning_rate = 1e-3\n",
    "args.experiment = 'experiment_2024-12-16_v2'\n",
    "args.dataset = 'moving-mnist-2digit-tr'\n",
    "args.num_frames = 8\n",
    "\n",
    "args.frame_dropout_probs = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "args.sampler_steps =             [2, 4, 6, 8, 10]\n",
    "\n",
    "args.resume = 'checkpoint_epoch_10.pth'\n",
    "\n",
    "args.debug = False\n",
    "args.device = device\n",
    "\n",
    "if args.debug:\n",
    "    args.experiment += '_debug'\n",
    "    args.dataset += '_debug'\n",
    "\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uj-wDg0HYsl"
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def _fig_bounds(x):\n",
    "    r = x//32\n",
    "    return min(5, max(1,r))\n",
    "\n",
    "def show_image(im, keep, ax=None, figsize=None, title=None, **kwargs):\n",
    "    \"Show a PIL or PyTorch image on `ax`.\"\n",
    "    cmap=None\n",
    "    # Handle pytorch axis order\n",
    "    if isinstance(im, torch.Tensor):\n",
    "        im = im.data.cpu()\n",
    "        if im.shape[0]<5: im=im.permute(1,2,0)\n",
    "    elif not isinstance(im, np.ndarray):\n",
    "        im=np.array(im)\n",
    "    # Handle 1-channel images\n",
    "    if im.shape[-1]==1:\n",
    "        cmap = \"gray\"\n",
    "        im=im[...,0]\n",
    "    alpha = 1\n",
    "    if keep == 0:\n",
    "        alpha = 0.5\n",
    "    if figsize is None:\n",
    "        figsize = (_fig_bounds(im.shape[0]), _fig_bounds(im.shape[1]))\n",
    "    if ax is None:\n",
    "        _,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(im, alpha=alpha, cmap=cmap, **kwargs)\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "    return ax\n",
    "\n",
    "def show_point(point_dict, ax=None):\n",
    "    \"Display points on the given axis `ax`.\"\n",
    "    pts = point_dict['center_points']\n",
    "    labels = point_dict['labels']\n",
    "    colors = point_dict['colors']\n",
    "\n",
    "    # Convert to numpy if points are in PyTorch tensor format\n",
    "    if isinstance(pts, torch.Tensor):\n",
    "        pts = pts.cpu().numpy()\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.cpu().numpy()\n",
    "    if isinstance(colors, torch.Tensor):\n",
    "        colors = colors.cpu().numpy()\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    # Convert to numpy if pts is a PyTorch tensor\n",
    "    if isinstance(pts, torch.Tensor):\n",
    "        pts = pts.cpu().numpy()\n",
    "    # Plot points\n",
    "    if pts is not None and len(pts) > 0:\n",
    "        for i in range(len(pts)):\n",
    "            ax.scatter(\n",
    "                pts[i, 0], pts[i, 1],\n",
    "                color=colors[i] if len(colors) > i else 'red',  # Use color from colors array or default to red\n",
    "                s=10,\n",
    "                label=f'Label {labels[i]}'\n",
    "            )\n",
    "            # Add text next to the point\n",
    "            ax.text(\n",
    "                pts[i, 0] + 0.5, pts[i, 1] + 0.5,  # Adjust offsets for better readability\n",
    "                str(labels[i]),\n",
    "                color=colors[i] if len(colors) > i else 'red',\n",
    "                fontsize=20\n",
    "            )\n",
    "\n",
    "\n",
    "def show_images(sequence_dict, points, nrows=1, ncols=None, titles=None, **kwargs):\n",
    "    \"Show all images `ims` as subplots with `rows` using `titles`.\"\n",
    "    ims = sequence_dict['sequence']\n",
    "    keep_frame_mask = sequence_dict['keep_frame_mask']\n",
    "    \n",
    "    if ncols is None:\n",
    "        ncols = int(math.ceil(len(ims)/nrows))\n",
    "    if titles is None:\n",
    "        titles = [None]*len(ims)\n",
    "    axs = plt.subplots(nrows, ncols, **kwargs)[1].flat\n",
    "    for im, keep, pts, t,ax in zip(ims, keep_frame_mask, points, titles, axs):\n",
    "        show_image(im, keep, ax=ax, title=t)\n",
    "        show_point(pts, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVTKkeevCZub"
   },
   "source": [
    "### Main -- Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QeFoYk3h_-Pd",
    "outputId": "93d55695-d4e7-44c8-c0f8-1f51c4133e81"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1000232/3650718039.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampler_steps=[] frame_dropout_probs=[]\n",
      "New computed stats for MovingMNIST: ([0.025074218750000002], [0.058953125])\n"
     ]
    }
   ],
   "source": [
    "args = Args()\n",
    "\n",
    "args.model = 'lstm'\n",
    "args.epochs = 15\n",
    "args.learning_rate = 1e-3\n",
    "args.experiment = 'experiment_2024-12-03_v1' #'experiment_2024-12-16_v2'\n",
    "args.dataset = 'moving-mnist-1digit-tr'\n",
    "args.num_frames = 8\n",
    "args.lstm_hidden_size = 64\n",
    "\n",
    "args.frame_dropout_probs = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "args.sampler_steps =             [2, 4, 6, 8, 10]\n",
    "\n",
    "args.resume = 'checkpoint_epoch_14.pth'\n",
    "args.decise = device\n",
    "\n",
    "model = build_model(args)\n",
    "\n",
    "output_dir = f'{args.output_dir}/{args.model}/{args.experiment}'\n",
    "checkpoint_path = Path(output_dir) / args.resume\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "model = model.to(args.device)\n",
    "model.eval()\n",
    "\n",
    "ds_default = build_dataset('val', args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sequence(sequence, targets_flat, out, ds_default, show_images_func, threshold=0.1, figsize=(20, 10), show_prediction=True):\n",
    "    \"\"\"\n",
    "    Process and display a sequence of frames with points and additional data.\n",
    "\n",
    "    Args:\n",
    "        sequence (Tensor): The input image sequence of shape (B, C, H, W).\n",
    "        targets_flat (list): List of targets containing points and labels for each frame.\n",
    "        out (dict): Output dictionary containing model predictions like logits and center points.\n",
    "        ds_default (object): Dataset object containing image size and other metadata.\n",
    "        show_images_func (callable): Function to display images (e.g., `show_images`).\n",
    "        threshold (float, optional): Confidence threshold for keeping queries. Default is 0.1.\n",
    "        figsize (tuple, optional): Size of the figure for display. Default is (20, 10).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    points_across_frames = []\n",
    "    keep_frame_mask = []\n",
    "\n",
    "    for i, t in enumerate(targets_flat):\n",
    "        # Keep frame mask\n",
    "        keep_frame_mask.append(t['keep_frame'].item())\n",
    "\n",
    "        # Extract and process points on the current frame\n",
    "        points_on_frame = {k: v.detach().cpu() for k, v in t.items()}\n",
    "        num_points = points_on_frame['labels'].size(0)\n",
    "        points_on_frame['colors'] = np.full(num_points, 'red', dtype=object)\n",
    "\n",
    "        if show_prediction:\n",
    "            print(show_prediction)\n",
    "            # Process logits and filter queries\n",
    "            logits_prob = out['pred_logits'].detach().cpu()[i].sigmoid()\n",
    "            logit_scores, logit_labels = logits_prob.max(-1)\n",
    "    \n",
    "            # Filtering based on threshold and background class\n",
    "            queries_keep = torch.logical_and(\n",
    "                logit_scores > threshold,\n",
    "                logit_labels != 10  # background class\n",
    "            )\n",
    "    \n",
    "            # Append filtered labels and center points\n",
    "            points_on_frame['labels'] = torch.cat([points_on_frame['labels'], logit_labels[queries_keep].detach().cpu()])\n",
    "            points_on_frame['center_points'] = torch.cat(\n",
    "                [points_on_frame['center_points'], out['pred_center_points'].detach().cpu()[i, queries_keep, :]]\n",
    "            )\n",
    "\n",
    "            # Add colors for new points\n",
    "            points_on_frame['colors'] = np.concatenate([points_on_frame['colors'], \n",
    "                                                        np.full(logit_labels[queries_keep].size(0), 'blue')])\n",
    "\n",
    "        # Rescale center points to the image size\n",
    "        for j, cp in enumerate(points_on_frame['center_points']):\n",
    "            if cp.size(0) != 0:\n",
    "                points_on_frame['center_points'][j] *= torch.tensor(\n",
    "                    [ds_default.img_size, ds_default.img_size], dtype=torch.float32\n",
    "                )\n",
    "\n",
    "        points_across_frames.append(points_on_frame)\n",
    "\n",
    "    # Call the show_images function to display\n",
    "    show_images_func({\n",
    "        'sequence': sequence.squeeze(0),\n",
    "        'keep_frame_mask': keep_frame_mask,\n",
    "    }, points_across_frames, figsize=figsize\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "zU86esBh-dIy",
    "outputId": "20a7702c-f212-4f10-b1ef-8b20432bb066"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for i, drop in enumerate(args.frame_dropout_pattern):\n",
    "#     targets_flat[i]['keep_frame'] = torch.tensor([int(drop)])\n",
    "    \n",
    "#     print(drop)\n",
    "\n",
    "# print(targets)\n",
    "\n",
    "\n",
    "idx = 55\n",
    "\n",
    "sequence, targets = ds_default[idx]\n",
    "#sequence_blind, targets_blind = ds_blind[idx]\n",
    "\n",
    "targets = [\n",
    "    [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "]\n",
    "\n",
    "sequence = sequence.unsqueeze(0).to(device)\n",
    "\n",
    "out, targets_flat = model(sequence, targets)\n",
    "\n",
    "show_sequence(sequence, targets_flat, out, ds_default, show_images, show_prediction=True)\n",
    "\n",
    "for i, drop in enumerate(args.frame_dropout_pattern):\n",
    "    targets_flat[i]['keep_frame'] = torch.tensor([1 - int(drop)])\n",
    "\n",
    "out, targets_flat = model(sequence, targets)\n",
    "\n",
    "show_sequence(sequence, targets_flat, out, ds_default, show_images, show_prediction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "fp4pOWV8K4pK"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds_blind' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      3\u001b[0m sequence, targets \u001b[38;5;241m=\u001b[39m ds_default[idx]\n\u001b[0;32m----> 4\u001b[0m sequence_blind, targets_blind \u001b[38;5;241m=\u001b[39m \u001b[43mds_blind\u001b[49m[idx]\n\u001b[1;32m      6\u001b[0m targets \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      7\u001b[0m     [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[1;32m      8\u001b[0m ]\n\u001b[1;32m     10\u001b[0m targets_blind \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     11\u001b[0m     [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets_blind]\n\u001b[1;32m     12\u001b[0m ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ds_blind' is not defined"
     ]
    }
   ],
   "source": [
    "idx = 10\n",
    "\n",
    "sequence, targets = ds_default[idx]\n",
    "sequence_blind, targets_blind = ds_blind[idx]\n",
    "\n",
    "targets = [\n",
    "    [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "]\n",
    "\n",
    "targets_blind = [\n",
    "    [{k: v.to(device) for k, v in t.items()} for t in targets_blind]\n",
    "]\n",
    "\n",
    "sequence = sequence.unsqueeze(0).to(device)\n",
    "sequence_blind = sequence_blind.unsqueeze(0).to(device)\n",
    "\n",
    "out, targets_flat = model(sequence, targets)\n",
    "out_blind, targets_flat_blind = model(sequence_blind, targets_blind)\n",
    "\n",
    "ade = AverageDisplacementErrorEvaluator(\n",
    "          output_dir=f'{output_dir}/test_ade_defalt_epoch-{epoch}.png',\n",
    "          prefix='default',\n",
    "          matcher=criterion.matcher,\n",
    "          img_size=ds_blind.img_size,\n",
    "        )\n",
    "\n",
    "\n",
    "ade_blind = AverageDisplacementErrorEvaluator(\n",
    "          output_dir=f'{output_dir}/test_ade_blind_epoch-{epoch}.png',\n",
    "          prefix='blind',\n",
    "          matcher=criterion.matcher,\n",
    "          img_size=ds_blind.img_size,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default_ADE_0': 0.5303423404693604,\n",
       " 'default_ADE_1': 0.2678205966949463,\n",
       " 'default_ADE_2': 0.4827596843242645,\n",
       " 'default_ADE_3': 0.43848681449890137,\n",
       " 'default_ADE_4': 0.26353681087493896,\n",
       " 'default_ADE_5': 0.2926582098007202,\n",
       " 'default_ADE_6': 0.6143808960914612,\n",
       " 'default_ADE_7': 0.46977436542510986}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ade.update(*postprocessors['trajectory'](out, targets_flat))\n",
    "ade.accumulate()\n",
    "r = ade.summary()\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default_ADE_0': 0.5303423404693604,\n",
       " 'default_ADE_1': 0.2678205966949463,\n",
       " 'default_ADE_2': 0.4827596843242645,\n",
       " 'default_ADE_3': 0.43848681449890137,\n",
       " 'default_ADE_4': 0.4977521598339081,\n",
       " 'default_ADE_5': 0.5593279600143433,\n",
       " 'default_ADE_6': 0.7625422477722168,\n",
       " 'default_ADE_7': 0.8984476923942566}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ade.update(*postprocessors['trajectory'](out_blind, targets_flat_blind))\n",
    "ade.accumulate()\n",
    "r = ade.summary()\n",
    "r"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "mmnist",
   "language": "python",
   "name": "mmnist"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
